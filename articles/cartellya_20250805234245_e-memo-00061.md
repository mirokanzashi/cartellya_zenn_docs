---
title: "A3C"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 方策勾配法とA3C手法を勉強する

## キーワード
方策勾配法, 方策勾配定理, オンポリシー, オフポリシー, 
Actor-Critic, アドバンテージ関数, A3C

## 学習内容

### 方策勾配法(Policy Gradient method)
- 強化学習において「報酬を最大化するようにエージェントの方策（ポリシー）を直接パラメータで最適化する手法
- メリット：
    - 連続行動空間に適応できる
    - 方策を直接最適化するため柔軟
- デメリット：
    - 勾配の分散が大きく学習が不安定
    - サンプル効率が低い（経験の再利用が難しい）
- Actor-Criticは方策勾配法を改良した手法である。$Q(s,a)$ をCriticが予測する（TD学習）ことで高効率にする


#### 方策(policy)
- 状態sに対して行動aをとる確率分布π(a∣s;θ) のこと
- 方策勾配法では、この方策πをパラメータθで表現し、報酬の期待値が最大になるようにθを更新する

#### 目的関数
- 報酬の期待値のこと
- 値を最大化するために、方策のパラメータθを勾配上昇で更新

$$
J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
$$


#### 方策勾配定理(Policy Gradient Theorem)
- 勾配を見積もり、方策を改善していく

$$
L(\theta) = - \sum_t \log \pi_\theta(a_t|s_t) \cdot G_t
$$



### オンポリシー(on-policy)
- 現在エージェントが使っている方策（ポリシー）と同じ方策を使ってデータを収集・学習する方法
- 強化学習において「方策（policy）」とは、状態𝑠に対してどの行動𝑎を選ぶかの確率分布𝜋(𝑎∣𝑠)のこと
- オンポリシーでは、「今まさに使っている方策」でデータを収集し、そのデータを使って「その同じ方策」を改善する
- メリット：方策の最新性が高く、理論的に一貫性がある
- デメリット：データ効率が悪い（収集したデータがすぐ使えなくなる）。リプレイバッファとの相性が悪い

```
[今の方策 π] → [行動選択・経験収集] → [πの改善] → [新しい π]
（↑ループ：毎回新しいポリシーでデータを収集）
```

#### オフポリシー(off-policy)との比較
| 分類         | データ収集の方策    | 学習する方策    |
| ---------- | ----------- | --------- |
| オンポリシー | 今使っている方策（π） | 同じ方策（π）   |
| オフポリシー | 過去や他の方策（μ）  | 学習中の方策（π） |

### Actor-Critic手法
- 「行動（ポリシー）を選ぶ役割（Actor）」と「その行動がどれだけ良かったかを評価する役割（Critic）」を分離して同時に学習する手法
- ActorとCritic役割を分けて協調的に学習するから「Actor-Critic」と呼ばれる

| コンポーネント    |   Actor           | Critic          |  
| ---------- | ------------------------------------ | ----------- | 
| 内容   | 方策ネットワーク $\pi(a\|s; \theta)$  | 価値ネットワーク $V(s; \theta_v)$や$Q(s,a)$ |
| 役割 | 行動の選択（エージェントの意思決定） | 行動の評価・勾配の提供 | 
| 目的 | より良い行動（高い報酬を得られる行動）を選べるように方策を改善 | 実際に得られた報酬から「行動は良かったか？」を判断 |

#### 損失関数
- Actorの損失（方策勾配）

$$
L_{\text{actor}} = - \log \pi(a_t|s_t; \theta) \cdot A_t
$$

$A_t$：アドバンテージ関数（行動が平均よりどれくらい良いか）

- Criticの損失（状態価値関数の誤差）

$$
L_{\text{critic}} = \left( R_t - V(s_t; \theta_v) \right)^2
$$

#### アドバンテージ関数
-  関数：A(s,a)
- アクターの方策更新に使う「行動の良さの指標」
- 一般的：$A(s, a) = Q(s, a) - V(s)$
- 近似として、TD誤差（1ステップ）を使う：$A_t \approx r_t + \gamma V(s_{t+1})-V(s_t)$

#### 流れ
1. 状態$s_t$に対して Actor が行動$a_t$を選ぶ
2. 環境が次状態$s_{t+1}$、報酬$r_t$を返す
3. Criticが現在の価値を予測し、TD誤差を計算
4. TD誤差を使って：
    - Criticの価値ネットワークを更新
    - Actorの方策を強化or弱化（勾配上昇）

#### 特徴
| 項目    | 内容                                                       |
| ----- | -------------------------------------------------------- |
| 長所  | オンポリシーでも学習可能（PPOなど）<br>方策ベースなので連続行動空間に対応しやすい         |
| 短所 | Criticが不安定だとActorの学習も悪化（共依存）<br>経験の効率が悪い（リプレイが使いにくい） |

### A3C(Asynchronous Advantage Actor-Critic)
- 強化学習のActor-Critic手法を並列非同期に実行することで、効率的かつ安定した学習を実現するアルゴリズム
- 非同期で複数スレッドが学習

#### 特徴
| 特徴    | 内容                                        |
| ----- | ----------------------------------------- |
| 手法の分類 | Actor-Critic：方策と価値関数を同時に学習            |
| 学習の方式 | 非同期並列学習：複数のエージェントが独立に環境を探索し、各自の経験から学習 |
| 利点    | 学習が高速・安定、経験の多様性が高い（経験再生を使わない）             |
| Replay不要 | 経験再生バッファなしでも安定に学習可能（オンポリシー対応）    |
| 高速な学習    | 並列スレッドによりデータ収集が早い                |
| 安定性      | 異なる方策の並列実行により、多様な経験が得られる         |
| 高次元にも対応  | CNNを使って画像ベースの強化学習にも対応可能（Atariなど） |


#### 構成要素
| モジュール         | 内容                              | 
| ------------- | ------------------------------- | 
| Actor     | 方策 (\pi(a \| s;\theta)) を学習（行動の選択確率） |
| Critic  | 価値関数 $V(s;\theta_v)$ を学習（状態の価値） | 
| Advantage | TD誤差をベースに、行動の「どれだけ良かったか」を定量化：A(s,a)   | 

#### なぜ非同期
- 従来の深層強化学習は経験再生を使ってバッチ学習したが：
    - オンポリシー手法（今の方策を改善する）では replay が使いづらい
    - 同じ方策で動くと経験に偏りが生じやすい
    - Actor-Critic に適した安定な学習が求められる
- A3Cで複数のエージェントスレッドが：
    - 各自のコピーの環境で動作
    - 独立に探索・学習を行う
    - 少しずつグローバルモデル（パラメータ共有）に更新を反映する

#### 損失関数
$$
L = L_{\text{policy}} + c_v L_{\text{value}} - c_e H(\pi)
$$

$c_v,c_e$はそれぞれ価値損失・エントロピー項の重み

- 方策勾配項（Actor部分）

$$
L_{\text{policy}} = -\log \pi(a|s; \theta) \cdot A(s, a)
$$

- 価値関数の損失（Critic部分）

$$
L_{\text{value}} = \left( R - V(s; \theta_v) \right)^2
$$

- エントロピー正則化項（探索促進）

$$
H(\pi) = - \sum_a \pi(a|s) \log \pi(a|s)
$$



#### 学習の流れ
1. グローバルネットワーク（ActorとCritic）を共有パラメータとして初期化
2. 複数のスレッドで並列に環境とやり取り
3. 各スレッドが：
    - 経験を貯める（数ステップ分）
    - ローカルネットで勾配を計算
    - 勾配をグローバルネットに適用
    - ローカルネットをグローバルに同期
4. 終了状態または最大ステップでリセットして再開