---
title: "誤差逆伝播"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 誤差逆伝播はどのようなものを理解する

#### モデル訓練のステップ
1. アーキテクチャの設計
2. 入力層設計
3. 中間層設計
4. 出力層設計
5. 誤差計算
6. **モデルに誤差を反映する**
7. 最適なモデルを手に入れる

## キーワード
誤差逆伝播, 学習係数

## 学習内容
### 誤差逆伝播法
新たな重みを計算するため、ネットワークの各層にさかのぼって、偏微分して勾配（重み）を求めていく計算手法

#### 移動量を求める
- 手順
    1. 誤差関数を偏微分して傾きを求める
    2. 誤差関数を重みで偏微分した値に学習係数をかけ、移動量を求める
    3. 誤差（E）が0に近づくように新たな重み（W）を求める

- 新たな重み= 現在の重み-移動量

$$
W^{(t+1)} = W^t - \varepsilon \nabla E
$$

W：重み（パラメータ）
t：現在の学習ステップ（イテレーション）
ε：学習係数
∇E：損失関数の勾配

:::message
**学習係数**：重みを調整する際の移動量を決定するために使用するパラメーターであること
学習係数は1より小さな値を設定するのが普通だが、あまりにも小さいと、
更新量も小さくなり、計算の回数が増え十分な学習ができないことがる
:::


#### 重みの適用
計算で求めた新たな重み(W)を先ほどのニューラルネットワークに当てはめて、再度出力された値と正解の値の誤差(E)を求め、その誤差(E)を0に近づけるように新たな重み(W)を計算する

#### 反復学習
- 重みの更新の実行回数を設定する
- 学習回数を増やしすぎると時間がかかったが、極端に少ないと、十分な学習ができない場合がある
- 学習係数の値を小さくし、学習反復回数を増やすと、より学習の精度を上げることができる


### 勾配の計算
勾配の計算=偏微分の計算を用いる

### 逆伝播の計算
下記の画像で説明する
![](/images/e-memo-00019_01.webp)
*出典：
澁谷直樹@キカベン, １層と２層のニューラルネットワークで誤差逆伝播法を詳しく説明してみる(2024), https://note.com/kikaben/n/nfb46c377cb1e*

#### 順伝播の場合
入力：x
線形変換：$z=W_x+b$
活性化：$y=\sigma(z)$
損失値：$L(y, y^*)=$

#### 逆伝播の場合
損失値を偏微分にする：$\frac{\partial L(y, y^*)}{\partial y}$
活性化値を偏微分にする：$\frac{\partial y}{\partial z}$
線形変換した値を偏微分にする：$\frac{\partial z}{\partial W_i}$, $\frac{\partial z}{\partial b}$

結果：

$$
\frac{\partial L}{\partial w_i}=\frac{\partial L}{\partial y}\cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w_i}
$$
$$
\frac{\partial L}{\partial b}=\frac{\partial L}{\partial y}\cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial b}
$$


これらの偏微分をベクトルとしてまとめたものが勾配（gradient）である

$$
\nabla L=\left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, \cdots, \frac{\partial L}{\partial w_n}, \frac{\partial L}{\partial b} \right)

$$