---
title: "中間層の活性化関数の微分"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 中間層の活性化関数の微分結果を覚える
- 逆伝播で各活性化関数の微分の特徴を勉強する

#### モデル訓練のステップ
1. アーキテクチャの設計
2. 入力層設計
3. 中間層設計
4. 出力層設計
5. 誤差計算
6. **モデルに誤差を反映する**
7. 重みの更新の設定をする
8. 最適なモデルを手に入れる

## キーワード
勾配消失

## 学習内容

### シグモイド関数の微分

$$
y=\frac{1}{1+e^{-x}}
$$

$1+e^{-x}=u$とおくと$y=u^{-1}$

微分すると、$\frac{du}{dx}=-e^{-x}$, $\frac{dy}{du}=-u^{-2}=-(1+e^{-x})^{-2}$

$$
\frac{dy}{dx}=\frac{dy}{du}\cdot\frac{du}{dx}=-(1+e^{-x})^{-2}\cdot-e^{-x}=\frac{e^{-x}}{1+e^{-x}}\cdot\frac{1}{1+e^{-x}}
$$

$\frac{e^{-x}}{1+e^{-x}}=\frac{1+e^{-x}}{1+e^{-x}}-\frac{1}{1+e^{-x}}=1-\frac{1}{1+e^{-x}}$で変形すると

$$
\frac{dy}{dx}=(1-\frac{1}{1+e^{-x}})\cdot\frac{1}{1+e^{-x}}=(1-y)\cdot y
$$

#### シグモイド関数の微分について
- 微分した値は最大は0.25までしか取ることができないため、誤差は次第に小さくなっていく。勾配消失問題を引き起ってしまう
- 逆伝播の際、入力層に近いところで学習が進みにくい(微分値の最大値が0.25のため、学習の収束が遅い)

### tanh関数の微分
$$
y = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

$$
y'=\frac{4}{(e^x+e^{-x})^2}=\frac{1}{cosh^2x}=1-y^2
$$

備考：$coshx=\frac{(e^x+e^{-x})}{2}$

:::message
微分の計算は少し複雑になったため、暗記しよう
:::

#### tanh関数の微分について
- 微分値の最大値が1なので、入力層近くにおける学習がシグモイド関数より進みやすい
- 出力が−１から１で出力が0にセンタリングされるため、学習の収束がシグモイド関数よりも容易
- シグモイド関数と同様、入力が大きく(小さく)なると勾配が消える

### ReLU関数の微分

$$
y=max(0,𝑥)
$$

$$
y'=
\begin{cases}
1 & \text{if } x \ge 0 \\
0 & \text{if } x < 0 \\

\end{cases}
$$

#### ReLU関数の微分について
- 0以下は微分の値が0
- 0より大きくなると常に微分の値が1になり、安定して誤差が伝播するようになった