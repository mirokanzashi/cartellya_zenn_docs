---
title: "正則化"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 過学習対策の一つ正則化についてを勉強する


#### モデル訓練のステップ
1. アーキテクチャの設計
2. 入力層設計
3. 中間層設計
4. 出力層設計
5. 誤差計算
6. モデルに誤差を反映する
7. 重みの更新の設定をする
8. **最適なモデルを手に入れる**

## キーワード
正則化項, L1正則化, ラッソ回帰, L2正則化, リッジ回帰,
ノルム, weight decay, ドロップアウト, ドロップコネクト

## 学習内容

### 正則化
- 過学習を防ぐために、誤差関数にペナルティ項を課して、関数を最小化する方法

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \cdot \mathcal{R}(\theta)

$$

$\mathcal{L}_{\text{data}}$：誤差関数
$\lambda$：正則化係数（ハイパーパラメータ）
$\mathcal{R}(\theta)$：モデルの複雑さに対する罰則


#### 正則化項
- 罰則項/ペナルティ項とも呼ばれる
- 過学習を防ぐために、誤差関数に加えられる関数である

### L1正則化
- モデルの重みの絶対値の総和にペナルティを加える方法

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \sum_{j=1}^{d} |\theta_j|
$$

$\mathcal{L}_{\text{data}}$：データ損失（例：MSEなど）
$\theta_j$：パラメータ（重み）
$\lambda$：正則化係数（大きくするほど重みに強い罰）


#### ラッソ回帰
- L1正則化を使った線形回帰手法

$$
\min_{\boldsymbol{w}} \left\{ \sum_{i=1}^{N} \left( y_i - \boldsymbol{w}^\top \boldsymbol{x}_i \right)^2 + \lambda \sum_{j=1}^{d} |w_j| \right\}

$$

第一項：通常の二乗誤差（MSE）
第二項：L1正則化項
λ：正則化の強さ

### L2正則化
- 重みの二乗和にペナルティを加える

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \sum_{j=1}^{d} w_j^2

$$

$\mathcal{L}_{\text{data}}$：データ損失（例：MSEや交差エントロピー）
$w_j$：パラメータ（重み）
$\lambda$：正則化係数（大きくするほど重みに強い罰）

#### リッジ回帰
- L2正則化を使った線形回帰手法

$$
\min_{\boldsymbol{w}} \left\{ \sum_{i=1}^{N} \left( y_i - \boldsymbol{w}^\top \boldsymbol{x}_i \right)^2 + \lambda \sum_{j=1}^{d} w_j^2 \right\}

$$

第一項：通常の回帰誤差（二乗誤差、MSE）
第二項：L2正則化項
λ：正則化の強さ

### L1とL2の違い
| 比較項目    | L1正則化（Lasso）      | L2正則化（Ridge）      |
| ------- | ------------------- | -------------------- |
| 罰則      | $\sum_{j=1}^{d} \lvert\theta_j\lvert$ | $\sum w_j^2$ |
| 効果      | 重みを0にする（特徴選択）   | 重みを小さく抑える（スムーズ化） |
| 出力される重み | 多くが0になる（スパース）       | 全ての重みが残る（縮小される）      |
| 幾何的特徴   | ダイヤ型制約領域 → 角で0にしやすい | 球型制約領域 → 0になりにくい     | 


### ノルム
- ベクトルや行列の「大きさ（長さ）」を表す数学的な量

####  L1ノルム（マンハッタン距離）
- 成分の絶対値の合計
- L1正則化で使用
- 特徴：スパース（重みが0になりやすい）



$$
\| \boldsymbol{x} \|_1 = \sum_{i=1}^{n} |x_i|

$$

#### L2ノルム（ユークリッド距離）
- 成分の2乗和の平方根
- L2正則化で使用
- 特徴：なめらかに重みを小さくする


$$
\| \boldsymbol{x} \|_2 = \sqrt{ \sum_{i=1}^{n} x_i^2 }

$$

#### 比較

| ノルムの種類 | 制約領域の形      |
| ------ | ------------------ |
| L1ノルム  | ダイヤ形（角あり） → スパース効果 |
| L2ノルム  | 円形（なめらか）           |

![](/images/e-memo-00027_01.png)
*出典：
MA-fn(A M), 最短でリッジ回帰とラッソ回帰を説明（機械学習の学習 #3）(2019), https://qiita.com/MA-fn/items/851f9fc50bda034cd5c6*


#### weight decay
- 学習時に重みの更新にペナルティをかけて、重みが大きくなりすぎないようにする
- 実質的に L2正則化 と同じ意味を持つ
- weight decayの強さ=正則化係数

### ドロップアウト（Dropout）
- ニューラルネットワークの過学習を防ぐための正則化手法
- 学習中にランダムに一部のニューロンを無効化（出力を0に）することで、モデルに依存性を持たせず、汎化性能を高める
- 毎回、違うネットワーク構造を使って学習するイメージ
- ランダムに選ばれたノードだけで順伝播・逆伝播を行うため、あるノードに頼りすぎることがなくなり、ロバストな（頑健な）特徴表現が得られる

### ドロップコネクト（DropConnect）
- ニューラルネットワークの過学習を防ぐための正則化手法
- ドロップアウトの発展形で、「ノードの出力」ではなく「重み」をランダムに無効化すること

#### 比較
| 比較項目         | ドロップアウト | ドロップコネクト |
| ------------ | ---------------- | --------------------- |
| ランダムに無効化する対象 | ノードの出力       | 重みそのもの        |
| 実行方法         | 出力ベクトルに0をかける     | 重み行列の一部を0にする          |
| 効果           | 複数のサブネットを学習      | 複数のサブウェイト構造を学習        |
| 実装難易度        | 比較的簡単（広く使われている）  | やや複雑（勾配の計算処理が少し難しい）   |
