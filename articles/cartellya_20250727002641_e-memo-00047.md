---
title: "GPT-n"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- GPTシリーズモデルを勉強する

## キーワード
GPT, 基盤モデル, Instruction-tuned model, Fine-tuned model,
Zero-shot Learning, Few-shot Learning, Prompt-based Learning

## 学習内容
### GPT(Generative Pre-Training)とは
- 量のテキストを読んで、「次の単語を予測する能力」を身につける学習方法
- モデルに「前の単語列」が与えられたとき、次に来る単語を予測するタスクで学習

#### 例
```
入力：私は昨日、映画館で  
モデルの予測：ポップコーン を 食べました。
```

- 「私は昨日、映画館で」までが コンテキスト（文脈）
- その後に続く「ポップコーン」「を」「食べました」などを1単語ずつ予測する


### GPTのモデル

#### 基盤モデル（base model）
- 事前学習のみが完了した**素の状態**のモデル
- 特定のタスクや指示（プロンプト）に最適化されていない、中立な言語予測モデル
- ただの「次の単語を予測するモデル」
- 例：GPT-2, GPT-3, GPT-4（事前学習のみ）
- 何故基盤モデルが重要
    - 汎用的で柔軟：さまざまなタスクに転用可能
    - ファインチューニングや指示チューニングの出発点
    - 学術研究やカスタムAI開発に不可欠

#### Instruction-tuned model（指示チューニング済み）
- 人間の指示（prompt）を理解できるように追加訓練されたもの
- タスク指向（翻訳・要約・質問応答など）に強い
- 多くの場合、RLHF（人間のフィードバックによる強化学習）を使う
- 例：ChatGPT（GPT-3.5, GPT-4）

#### Fine-tuned model（タスク別微調整モデル）
- 特定の分野（医療、法律、金融、コードなど）向けにさらに学習したモデル
- Codex（コード用GPT）、MedGPT（医療用GPT）など


###  Zero-shot Learning（ゼロショット学習）
- 例を何も与えず、指示だけでタスクをこなす

```
Q: 英語で「ありがとう」は何ですか？
A: 
```

GPTは事前学習で「ありがとう → Thank you」という知識を持っているので、例がなくても答えられる

### Few-shot Learning（少数例学習）
- 数個の例（入力と出力のペア）を与えて、パターンを学ばせてからタスクを行う
- タスクに応じた「例示」だけでモデルの振る舞いが変わるタスクに応じた「例示」だけでモデルの振る舞いが変わる
- **モデル本体は更新しない**（学習というより推論）

```
日本語：こんにちは → 英語：Hello  
日本語：ありがとう → 英語：Thank you  
日本語：おはよう → 英語：
```

GPTは上の例を参考にして、「Good morning」と正しく続けられる


### Prompt-based Learning（プロンプト学習）
- プロンプト（指示文）を工夫して、モデルの出力をコントロールする学習のスタイル全般を指す
- Few-shotやZero-shotもPrompt-based Learningの一種と見なされる

```
あなたはプロの翻訳者です。以下の日本語を英語に訳してください：
「今日は暑いですね」
```

プロンプト（指示文）の工夫でGPTに望む行動をさせる方法

:::message
**プロンプト**
- AI（特にGPTのような言語モデル）に対して出す指示文のこと
- 役割:：
    - 出力をコントロールする
    - タスクを指定する（翻訳、要約、質問応答、文生成、ロール設定など）
    - 文体や立場を決める（例：「あなたは医者です」）
:::


### BERTとGPTの比較
| 項目           | BERT                     | GPT                   |
| ------------ | --------------------------------- | ----------------------- |
| 学習方向         | 双方向（上下文を同時に読む）      | 一方向（左から右に読む） |
| 学習方式（事前学習）   | Masked Language Modeling + Next Sentence Prediction     | Next Token Prediction（次単語予測）             |
| タスク適性        | 理解系（分類、質問応答、文間関係）  | 生成系（文章生成、対話、翻訳、要約）  |
| 応答形式         | 固定形式（分類やスパン抽出など）         | 自由形式（自然文をそのまま生成）      |
| ファインチューニング必要 | 基本は必要（タスクに応じて微調整）             | 指示・プロンプトで多くのタスクに対応可能       |
| 代表的モデル       | BERT, RoBERTa, ALBERT, DistilBERT     | GPT-2, GPT-3, GPT-4, ChatGPT, GPT-neo など |
| 開発元          | Google               | OpenAI（GPTシリーズ）            |
| 文の理解 | ◎（分類・抽出が得意）  | ◯（構文理解は強いが分類には向かない） |
| 文の生成 | ×（生成できない）    | ◎（自然な文を自由に生成できる）    |
| 質問応答 | ◎（スパン抽出で答える） | ◎（自然文で答える）          |


- BERT = 「文を深く理解して、分類・抽出が得意な読解型モデル」
- GPT = 「文をスムーズに生成できる、柔軟な作文型モデル」