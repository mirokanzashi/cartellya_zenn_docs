---
title: "モデルの近似"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- モデルの近似の手法を勉強する

## キーワード
モデルの近似, 局所的な解釈, 大域的な解釈, LIME, 協力ゲーム理論,
SHAP, Shapley値

## 学習内容
### モデルの近似
- 複雑で直接理解しにくい元のモデル（ブラックボックス）を、より単純で人間が理解しやすい別のモデルで置き換えること
- 元のモデルそのものを変えるのではなく、入力と出力の関係を真似するという点がポイントである
- 目的
    - 可視化・説明：元のモデルがどう判断しているかを人間が理解できるようにする
    - 検証：モデルが本当に妥当なルールで予測しているか確認する
    - 監査・規制対応：「なぜこの結果になったか」を第三者に説明する必要がある場合
- 種類
    - 大域的近似：元モデル全体の挙動を単純なモデルで再現
    - 局所的近似：特定の入力サンプルの近くでのみ挙動を再現
- 注意点
    - 近似モデルは「正確に真似している」とは限らない
    - 局所的近似はその入力以外では意味がない
    - 大域的近似は単純化のために精度を犠牲にすることがある

### 局所的な解釈(Local Interpretability)
- XAIでモデルをどの範囲で説明するかを区別するための概念の一つ
- 特定の予測や入力サンプルに対して「なぜこの入力がこう予測されたのか」を説明する
- 木の「この枝」だけにズームして説明するイメージ
- 例：
    - 医療診断AIで「患者Aの画像がなぜ『悪性』と分類されたのか」を説明
    - LIME、SHAPの個別事例可視化、Grad-CAMなど


#### 特徴
- 1つの事例ごとに異なる説明が得られる
- モデル全体の動きは分からない
- 人間が「この結果の根拠」を知りたい場面で有効

### 大域的な解釈(Global Interpretability)
- モデル全体の挙動・傾向を説明する
- 「どの特徴が一般的に重要か」や「モデルのルール全体」を理解する
- 森全体の地図を眺めるようなイメージ
- 例：
    - 決定木の全構造表示
    - 全特徴量のSHAP値平均プロット
    - 特徴重要度ランキング（Feature Importance）

#### 特徴
- モデルの全体像・傾向が把握できる
- ある特定の予測事例の理由は分かりにくい
- モデル監査や規制対応、特徴選択などに有効

#### 比較
| 項目    | 局所的な解釈                   | 大域的な解釈                               |
| ----- | ------------------------ | ------------------------------------ |
| 範囲    | 特定の予測結果                  | モデル全体                                |
| 目的    | 個別判断の理由理解                | 全体的傾向の理解                             |
| 代表手法  | LIME, Grad-CAM, SHAP（個別） | 決定木可視化, Feature Importance, SHAP（平均） |
| メリット  | 事例ベースで説得力                | 全体像を把握できる                            |
| デメリット | モデル全体像がわからない             | 個別事例の詳細理由は不明                         |


### LIME(Local Interpretable model-agnostic explanations)
- 任意のブラックボックスモデルに対して、特定の予測結果（局所的解釈）を人間にわかる形で説明する手法
- 非線形の複雑なモデルを、線形回帰で近似するというアプローチ
- 特徴
    - モデルを問わず、様々な機械学習モデルに適用可能
    - 局所説明に特化
    - モデル非依存：内部構造にアクセスせず使える
- 短所
    - ランダムに生成した近傍データが不自然な場合、説明が不正確になる
    - 同じ入力でも近傍生成のランダム性で結果が変わる
    - 局所説明なのでモデル全体の理解には不向き

#### 流れ
対象：入力xに対して、モデルfが予測f(x)をした理由を説明したい

1. 近傍データ生成：元の入力xを少しずつ変化させたサンプルを大量に作る（特徴をランダムに欠損・置換）
2. 元モデルで予測：各近傍サンプルに対してfの予測値を取得
3. 重み付け（距離ベース）：元のxに近いほど高い重みを与える
4. 解釈可能モデルで近似：決定木や線形回帰などシンプルなモデルgを、重み付きデータで学習
5. 説明生成：近似モデルgの係数やルールを説明として提示

### 協力ゲーム理論(Cooperative Game Theory)
- 複数のプレイヤーが協力してある成果（利益や報酬）を得るときに、「その成果をどう公平に分配するか」を分析するゲーム理論の一分野
- 考え方
    - ゲームの結果（報酬）は複数人の協力によって生まれる
    - 各プレイヤーは単独では成果を得られないこともある
    - 成果を分ける方法は1つではなく、公平性の基準をどう定めるかが重要

####  代表的な解概念（成果の分け方）
- Shapley値
    - 全ての参加順序を考慮し、平均的な限界貢献度で分配
    - 公平性の4つの公理（効率性・対称性・無関係者・加法性）を満たす
    - SHAP（XAI）の理論的基盤
- コア（Core）
    - どの部分集合のプレイヤーも「もっと稼げる」組み合わせがない状態の分配
    - 安定性重視
- ナッシュ交渉解（Nash Bargaining Solution）
    - プレイヤー間の交渉モデルから最適分配を導く

### SHAP(SHapley Additive exPlanations)
- 機械学習モデルの予測を特徴量ごとに「どれだけ寄与したか」に分解して説明する手法
- 「特徴量をプレイヤー、モデル予測を成果」とみなして協力ゲーム理論を適用
- 特徴量の寄与度＝Shapley値として計算することで、公平な重要度配分を実現
- Shapley値を求めることで、各特徴量の増減が、どれだけ予測に影響を与えるかを可視化する
- 従来のFeature Importanceはモデル依存や順序依存の問題があった→ ゲーム理論に基づくShapley値で公平性を保証
- 局所的解釈も大域的解釈も可能
    - 大域説明（全体傾向）：特徴量重要度のランキングや散布図で表示
    - 局所説明（1つの予測）：棒グラフで「予測値を押し上げた特徴」「押し下げた特徴」を表示
- 注意点
    - 正確なShapley値計算は計算量が指数的（特徴量が多いと近似が必須）
    - 「公平性」は定義通りだが、ユーザーが理解できるかは別問題

#### 基本の考え方
- 複数の特徴量をプレイヤー、モデルの予測をゲームの報酬とみなす
- ある特徴量の寄与は「その特徴が参加したことで予測がどれだけ変わったか」
- 寄与度は全ての順列で平均することで、公平に計算

### Shapley値(ShapleyValue)
- 協力ゲーム理論において複数のプレイヤーが協力して得た成果を「公平」に分配するための数学的な方法である
-  XAIでの応用
    - プレイヤー＝特徴量
    - 成果＝モデルの予測値
    - Shapley値＝特徴量の寄与度（公平に配分）

#### 背景
- 協力ゲームでは、誰がどれだけ成果に貢献したかを測る必要がある
- 貢献度は単純に「単独の成果」で測れない（特徴量やプレイヤーは組み合わせ効果を持つ）
- Shapley値は「全ての順番で見た平均的な限界貢献度」を使うことで、公平な分配を保証

#### 公平性の4つの公理
- Shapley値は次の4つの性質をすべて満たす唯一の分配方法である

1. 効率性（Efficiency）：全員のShapley値の合計が総成果に等しい
2. 対称性（Symmetry）：貢献度が同じプレイヤーは同じ報酬を得る
3. 無関係者（Dummy Player）：貢献しないプレイヤーは報酬0
4. 加法性（Additivity）：複数ゲームを足し合わせたとき、Shapley値も加法的に計算できる

#### 例
プレイヤーA,B,C が協力し、成果vが以下のとき：
| チーム     | 成果 $v(S)$ |
| ------- | --------- |
| {A}     | 10        |
| {B}     | 0         |
| {C}     | 0         |
| {A,B}   | 20        |
| {A,C}   | 20        |
| {B,C}   | 10        |
| {A,B,C} | 30        |


Shapley値を計算すると（数式省略）：
- $𝜙_𝐴$=15
- $𝜙_B$=7.5
- $𝜙_C$=7.5
- 全員合計＝30