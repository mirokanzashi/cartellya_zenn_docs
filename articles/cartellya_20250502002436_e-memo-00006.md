---
title: "情報理論：エントロピーとダイバージェンス"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2

## キーワード
条件付きエントロピー, 結合エントロピー, クロスエントロピー, 
交差エントロピー, KLダイバージェンス, JSダイバージェンス

## 学習内容

### 条件付きエントロピー
- ある変数Yが与えられたときに、別の変数Xの不確実性（エントロピー）を表します
- YがXを完全に決定：H(X|Y)=0（不確実性0）
- Yを知ってもXは予測できない：H(X|Y)=H(X)（情報は得られていない）


$$
H(X \mid Y) = - \sum_{y \in \mathcal{Y}} P(y) \sum_{x \in \mathcal{X}} P(x \mid y) \log P(x \mid y)
$$

### 結合エントロピー
- 2つ以上の確率変数が同時に取る値の不確実性の総量を表します
- 単独のエントロピーが1つの変数の不確実性を測るのに対して、結合エントロピーは「同時に観測される全体の不確実性」を評価します
- XだけでなくYも含めた全体の不確実さを測ります
- 2つの変数が独立：H(X,Y)=H(X)+H(Y)
- 2つの変数が依存している場合：H(X,Y)<H(X)+H(Y)

$$
H(X, Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(x, y) \log P(x, y)
$$


### 交差エントロピー
- クロスエントロピーとも呼ばれます
- ある「真の確率分布P」と「予測した確率分布Q」の間の差（不確実性）を測る指標です
- 値が小さいほど予測が正確です
- P=Qのときに最小（=エントロピーに一致）

$$
H(P, Q) = - \sum_{x \in \mathcal{X}} P(x) \log Q(x)

$$

- X：クラスの集合


### KLダイバージェンス
- 相対エントロピーとも呼ばれます
- ある分布Qと真の分布Pがどれだけ事なぅている化を示す指標です
- $D_{\text{KL}}$は非負の値となります

$$
\begin{aligned}
D_{\text{KL}}(P \parallel Q) &= \int P(x) \log \left( \frac{P(x)}{Q(x)} \right) dx
\\&=-\int P(x) \log Q(x)dx-\left(-\int P(x)\log P(x)dx\right)
\end{aligned}
$$

![](/images/e-memo-00006_01.png)
*出典：
yusuke_ujitoko, KL divergenceとJS divergenceの可視化(2017), https://yusuke-ujitoko.hatenablog.com/entry/2017/05/07/200022*

### JSダイバージェンス
- KLダイバージェンスと同様に、分布間の差異を計算します
- KLダイバージェンスは対称性を持たないため、距離として扱えません
- $D_{\text{KL}}(P \parallel Q)$と$D_{\text{KL}}(Q \parallel P)$のどちらを使うかに
よって結果が変わります
- KLダイバージェンスを対象化、平滑化します

$$
M(x)=\frac{1}{2}P(x)+\frac{1}{2}Q(x)の時、\\
\\
D_{\text{JS}}(P \parallel Q)=\frac{1}{2}D_\text{KL}(P \parallel M)+\frac{1}{2}D_\text{KL}(Q \parallel M)
$$

:::message
$M(x)=\frac{1}{2}P(x)+\frac{1}{2}Q(x)$の$\frac{1}{2}$は対称になるため
:::