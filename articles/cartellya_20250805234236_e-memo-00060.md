---
title: "DQN"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- DQN手法を勉強する

## キーワード
TD学習, Q学習, Experience Replay, DQN

## 学習内容

### TD学習(Temporal Difference Learning)
- 実際の報酬と予測の差を用いて、逐次的に価値を更新するアルゴリズム
- 経験に基づき、将来の予測（価値）を段階的に修正する学習法

#### 特徴
| 特徴         | 説明                                          |
| ---------- | ------------------------------------------- |
| モデルフリー   | 状態遷移確率や報酬関数は不要                              |
| オンライン学習可 | 経験したらすぐに更新可能                                |
| バッチ不要    | エピソードを待たず逐次更新できる                            |
| 予測ベース    | 実際の将来報酬の合計を待たず、予測された値で更新できる |


#### 数式
$$
V(s_t) \leftarrow V(s_t) + \alpha \left[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right]
$$

| 記号           | 意味                                 |
| ------------ | ---------------------------------- |
| $s_t$        | 時刻 $t$ における状態                      |
| $r_{t+1}$    | 状態 $s_t$ で行動をとった後、時刻 $t+1$ に受け取る報酬 |
| $\gamma$     | 割引率（0〜1）：将来の報酬の重要性                 |
| $\alpha$     | 学習率（0〜1）：どれだけ更新するかの大きさ             |
| $V(s_t)$     | 現在の状態 $s_t$ の価値（予測）                |
| $V(s_{t+1})$ | 次の状態の価値（予測）                        |

1. 今の状態$s_t$に対する現在の価値$V(s_t)$を持っている
2. その状態で行動をとり、報酬 $r_{t+1}$を得て、次の状態$s_{t+1}$に遷移する
3. 次の状態$s_{t+1}$の価値$V(s_{t+1})$を使って、将来の報酬を予測する
4. 現在の予測 $V(s_t)$ と、次状態からの予測（＋即時報酬）との差をとり、その差分（TD誤差）をもとに更新する

- TD誤差：予測のズレを使って価値関数を修正する
    - $\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$


### Q学習(Q-learning)
- 行動価値関数（Q関数）を逐次更新して最適な方策を学習する方法
- 状態sにいて行動𝑎をとったとき、将来的にどれだけの報酬が得られるか
- 例：迷路ゲーム。
    - Q学習を使うことで、「どのマスでどの方向に進めばよいか」が学習できる

#### 数式

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
$$

​
- $s_t$：現在の状態
- $a_t$：選んだ行動
- $r_{t+1}$：行動後に得た報酬
- $s_{t+1}$：次の状態
- α：学習率（0 < α ≤ 1）
- γ：割引率（将来の報酬の重み）

#### 学習の流れ
1. Qテーブルを初期化（全状態×行動に対するQ値を初期化）
2. 環境の中でエージェントが行動を繰り返す
3. 状態sで行動aを選ぶ
4. 次の状態𝑠′、報酬rを得る
5. 上記の更新式でQ(s,a) を更新
6. 状態を𝑠′に移動して繰り返す

#### 特徴
| 項目        | 内容                                 |
| --------- | ---------------------------------- |
| モデル不要     | 環境の遷移確率や報酬関数を知らなくてもOK（モデルフリー）      |
| オフポリシー    | 実際の行動と更新する方策が異なっていても学習できる          |
| 最適方策が得られる | 十分な探索と反復で最適なQ関数に収束する理論保証あり（有限状態空間） |

### Experience Replay（経験の再利用）
- 強化学習（特に深層強化学習）において、エージェントが過去に経験した状態・行動・報酬・次状態の組（トランジション）を記憶しておき、その記憶からランダムにサンプルを抽出して学習に使う手法

#### 目的
- 強化学習の安定化とデータ効率の向上


| 目的          | 説明                                                                             |
| ----------- | ------------------------------------------------------------------------------ |
| 相関の除去   | 時系列データ（連続した経験）には強い相関があり、これはニューラルネットの学習に悪影響。Replayではランダムに抽出して、学習データの独立性を保つ。 |
| データ効率向上 | 一度の経験を何度も使えるので、少ない経験で高い学習効率が得られる。                                              |
| 安定した学習  | 最新の経験だけでなく、過去の多様な状況も学習に反映できるため、学習が安定しやすい。                                  |

#### リプレイバッファの構造(Replay Memory)
- 通常は固定サイズのキュー(FIFO)
- 古い経験から順に削除し、新しい経験を追加していく
- 例：100,000件の経験を保持し、学習のたびに64件ずつサンプリング


### DQN(Deep Q-Network)
- 強化学習のQ学習に深層ニューラルネットワークを組み合わせて、高次元・連続的な状態空間でも最適な行動価値関数を学習できるようにした手法
- 状態を入力に取り、行動価値Q(s,a) を出力するニューラルネットワークで近似
- ニューラルネットワークのパラメータを勾配降下法で更新して学習する

#### 特徴的な技術
| 技術名                         | 内容                                         | 役割               |
| --------------------------- | ------------------------------------------ | ---------------- |
| Experience Replay | 過去の経験をバッファに蓄え、ランダムにサンプリングして学習              | データの相関を減らし、学習安定化 |
| ターゲットネットワーク             | 一定間隔で固定されたパラメータのネットワークを使い、学習対象ネットワークの更新に利用 | 目標値の安定化、発散防止     |
| ε-greedy方策          | 探索と活用のバランスをとるために、ランダム行動を一定確率で行う            | 十分な探索を保証         |

- DQNでExperience Replay使用例
    - 経験をためる：各タイムステップで$(s_t,a_t,r_{t+1},s_{t+1})$をリプレイメモリ（バッファ）に保存
    - 学習のときに：バッファからランダムにミニバッチをサンプリングして、ニューラルネットの更新に使う


#### 学習ステップ
1. 環境と状態sを初期化
2. 行動aをε-greedy方策で選択
3. 報酬r、次状態𝑠′を観測し経験をバッファに保存
4. バッファからランダムに経験を抽出し、損失関数を計算
5. 損失の勾配でパラメータθを更新
6. 一定ステップごとにターゲットネットワークを更新 $𝜃^−←𝜃$
7. 状態を𝑠′に更新し繰り返す