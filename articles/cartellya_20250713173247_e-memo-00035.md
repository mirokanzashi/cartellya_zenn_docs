---
title: "リカレントニューラルネットワーク"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- リカレントニューラルネットワークはどのようなものを理解する

## キーワード
リカレントニューラルネットワーク, BPTT, 双方向RNN

## 学習内容

### リカレントニューラルネットワーク（RNN: Recurrent Neural Network）
- 時系列データや系列データ（シーケンス）を扱うためのニューラルネットワークの一種である
- 過去の情報を記憶して、「現在の出力は過去の入力にも依存する」という構造を持つ
    - 時間方向に「再帰（Recurrent）」する構造が RNN の名前の由来
- 従来のニューラルネットワークは、入力と出力が固定長で、時間的な順序や前後関係を扱えない
    - 例えば、三日前の株価、二日前の株価、昨日の株価、今日の株価全部入力層で一律入力した。時間の関係が反映できない

:::message
**時系列データ：** ある特定の情報を、連続的に(または一定間隔で)観測して得られたデータの事
:::

### RNNの特徴
- 中間層で「隠れ状態（hidden state）」を内部で持ち、過去の情報を保持しながら逐次的に処理する
- ある瞬間に出力した時の中間層をまた次の時の中間層に入力することで、過去の入力を現在の入力に引き継がせることができる
- 時系列・文脈情報を保持できるがRNNの長所である


![](/images/e-memo-00035_01.png)
*出典：
LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. https://doi.org/10.1038/nature14539*

𝑥:入力
𝑜:出力
𝑠:隠れ層
𝑈:入力の重み
𝑊:次の隠れ層への重み
𝑉:出力の重み


![](/images/e-memo-00035_02.jpg)
*出典：一色政彦, RNN（Recurrent Neural Network： 再帰型ニューラルネットワーク）とは？(2019), @IT, https://atmarkit.itmedia.co.jp/ait/articles/1901/06/news050.html*

![](/images/e-memo-00035_03.jpg)
*出典：植田 佳明, 再帰型ニューラルネットワークの「基礎の基礎」を理解する　～ディープラーニング入門｜第3回, IS magazine No.14（2017）, https://www.imagazine.co.jp/%E5%86%8D%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E3%80%8C%E5%9F%BA%E7%A4%8E%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%80%8D/*

この三つの画像で、中間層の出力を次の中間層の入力に引き継がせることを理解して、同時に、再帰であることも理解できた




### 数式

LeCunさんの画像を見て、数式を勉強する

$𝑎^{(𝑡)}=𝑏+𝑊𝑠^{(𝑡−1)}+𝑈𝑥^{(𝑡)}$
状態の前処理：前の状態$s^{(t-1)}$ と現在の入力 $x^{(t)}$ から次の状態の「準備」をする

$𝑠^{(𝑡)}=tanh(𝑎^{(𝑡)})$
現在の隠れ状態の更新：非線形変換によって情報を圧縮し、次の隠れ状態を計算

$𝑜^{(𝑡)}=𝑐+𝑉𝑠^{(𝑡)}$
出力の前処理（スコア計算）：隠れ状態から「出力の元となるスコア」を計算する

$\hat{y}^{(t)}=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑜^{(𝑡)})$
出力の確率化（分類）：スコア $o^{(t)}$ を各クラスの確率に変換


| 記号           | 意味                 |
| ------------ | ------------------ |
| $x^{(t)}$       | 時刻 $t$ の入力         |
| $s^{(t)}$       | 隠れ状態（メモリ・文脈）       |
| $o^{(t)}$       | 出力スコア（線形変換後）       |
| $\hat{y}^{(t)}$ | 出力の確率（Softmaxで正規化） |
| $U, W, V$    | 各種重み行列（学習される）      |
| b    | バイアス項      |



### BPTT(back-propagation through time)
- RNNで使う誤差逆伝播法。時間軸方向に展開したニューラルネットワークの誤差逆伝播法
- 時間軸方向に展開する事で、長いニューラルネットワークとみなす事が出来る。つまり誤差は、最後の時刻から、最初の時刻に向かって誤差が伝播していく

#### 問題点
- 時系列が長期になると、勾配が消失（または爆発）していく傾向があり、過去の情報が消失してしまう
- 並列処理ができないため、学習に時間がかかる


### 双方向RNN（Bidirectional RNN, BiRNN）
- 入力系列を「前から後ろ」だけでなく「後ろから前」でも処理するRNNの構造
- 言語処理で、文脈が後から分かる文があるので、双方向にすれば、前後の両方の情報を使って判断できる