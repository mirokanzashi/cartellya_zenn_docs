---
title: "パラメータ初期化戦略"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 重み初期化する理由と手法を学ぶ


#### モデル訓練のステップ
1. アーキテクチャの設計
2. 入力層設計
3. 中間層設計
4. 出力層設計
5. 誤差計算
6. モデルに誤差を反映する
7. 重みの更新の設定をする
8. **最適なモデルを手に入れる**

## キーワード
Xavier初期化, Glorot初期化, He初期化, Kaiming初期化

## 学習内容

### 重み初期化の理由
- ニューラルネットワークでは、重みの初期値が悪いと学習が進まない
- 勾配消失・爆発問題を防ぐ
- 適切な初期化をすることで、活性化関数とネットワークの深さに応じたスケール調整ができる

:::message
**スケール：**値の分散（ばらつきの大きさ）や絶対値の大きさ
:::

### Xavier初期化(Glorot初期化)
- 対象：主に tanh や sigmoid のような対称な活性化関数
- 目的：前の層から出力される分散と次の層への入力の分散が等しくなるようにする

### He初期化(Kaiming初期化)
- 対象：ReLU系（ReLU, LeakyReLU, ELUなど）の活性化関数
- 目的：ReLUを使うときの出力のスケール保持、勾配消失の防止