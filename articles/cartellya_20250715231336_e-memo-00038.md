---
title: "RNNの系列変換"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- RNNの系列変換について、seq2seqとアテンション機構を勉強する

## キーワード
sqe2seq, エンコーダ,デコーダ, アテンション機構

## 学習内容

### Sequence to Sequence(sqe2seq)
- 入力の系列データ（sequence）を、出力の系列データに変換するためのニューラルネットワークアーキテクチャ
- 入力シーケンスを固定長のベクトル表現にエンコードし、その後、その表現を使用してターゲットシーケンスを生成するためにデコードする構造
- エンコーダ + デコーダ の2つのRNNで構成
- コンテキストベクトルcに全情報を詰め込むため、長い文に弱い

![](/images/e-memo-00038_01.png)
*出典：
hilinker, どやの情弱克服ブログ, よくわかる注意機構（Attention）(2018),https://hilinker.hatenablog.com/entry/2018/12/08/002003*


#### Seq2Seqの処理
- 時系列の入力${𝑥^1,𝑥^2,…𝑥^𝑇}$に対して、出力{𝑦^1,𝑦^2,…𝑦^𝑇′}を生成するモデルを考える。
- 構造全体の学習は最尤法で行われる
- モデルが正解系列を出す確率を最大化するための損失関数は以下である

$$
E(w) = -\frac{1}{N} \sum_{n=1}^{N} \left[ \log \left\{ P\left(y_n^1, y_n^2, \ldots, y_n^{T'} \mid x_n^1, x_n^2, \ldots, x_n^T ; w \right) \right\} \right]

$$

| 記号                       | 意味                              |
| ------------------------ | ------------------------------- |
| $N$                      | サンプル数（バッチの件数）                   |
| $x_n^1, ..., x_n^T$      | 入力系列（n番目のサンプルの入力）               |
| $y_n^1, ..., y_n^{T'}$   | 出力系列（n番目のサンプルの正解出力）             |
| $w$                      | モデルのパラメータ（重みなど）                 |
| $P(...)$                 | モデルが「この出力を出す確率」                 |
| $\log P(...)$            | 対数尤度（log-likelihood）            |
| $- \frac{1}{N} \sum ...$ | 平均負対数尤度 → 交差エントロピー損失と本質的に同じ |



### エンコーダとデコーダ
- エンコーダ：入力系列を読み取り、文全体を1つのベクトルに圧縮
- デコーダ：ベクトルから、別の系列として出力を生成
- Seq2Seqのような“入力系列 → 出力系列”の変換タスクにRNNを使う際、その役割を分ける必要があるため、RNNを「エンコーダ」と「デコーダ」に分けて構成するのです。

#### なぜ1つのRNNでは不十分なのか
- 入力と出力が非対称（長さが異なる、言語が異なる、構造が異なる）
- RNNは「逐次的」処理なので、入力と出力を同時に処理しにくい


### アテンション機構(Attention Mechanism)
- Seq2Seqの問題を解決するため
    - 情報が1ベクトルに押し込まれて失われる→長い文に弱い
    - 単語ごとの対応が曖昧になる→出力ごとに柔軟な参照ができない
    - 「出力ごとに入力系列の重みを変える」という発想がアテンション機構である
- 出力の各時点ごとに、入力のどの部分が重要かを自動で判断する仕組み
    - 例えば、「It is cold today」→「今日は寒い」を翻訳する時に、「寒い」という単語を生成するとき、"cold" に強く注意（=重みをかける）

#### query,key,value
| 役割        | 例え                 | attentionにおける意味 |
| --------- | ------------------ | ---------------------|
| Query | ユーザーの検索ワード（探したいもの） | 問い合わせ |
| Key   | データベース内のタグ・特徴      | どれだけ一致しているかをスコアにする |
| Value | 実際のデータ（記事・画像など）    | そのスコアに応じて加重平均する |

#### 数式
$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
$$

| 記号        | 意味                |
| --------- | ----------------- |
| $Q$：Query | 注目したい対象（出力位置）     |
| $K$：Key   | 比較対象（入力の各単語の特徴）   |
| $V$：Value | 実際に取り出す情報（同じく入力）  |
| $d_k$     | スケーリング定数（keyの次元数） |

 