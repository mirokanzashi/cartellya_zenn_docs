---
title: "パラメータ推定：ベイズ則と尤度"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- ベイズ則と尤度に関する知識とモデルを理解します

## キーワード
ベイズ則,最尤推定,最尤推定量,尤度関数,対数尤度関数
事前確率, 事後確率, ナイーブベイズ, MAP推定

## 学習内容
### 事前確率と事後確率
#### 事前確率
新しい情報を得る前に仮説が正しいと考えられる確率のことです

#### 事後確率
新しい情報を得た後に仮説が正しいと考えられる確率のことです

#### 例
- ある病気にかかっている人は全体の1% 　→　「事前確率」
- 検査を受けて「陽性だった」とする。この情報を得て「本当に病気か？」を考え直す　→　新しい情報「陽性だった」
- 検査の結果は陽性だが、本当に病気である確率　→　「事後確率」



### 尤度関数
尤度：あるパラメータのもとで、観測されたデータの「尤もらしさ」（このデータがこのパラメータのもとで起きる確率）
尤度関数：観測された「全部の」データのパラメータを変数として扱う数式

:::message
パラメータ（θ）：確率モデルの性質を決める変数のことです

例：コイン投げ
　モデル：表が出る確率𝜃のコイン
　パラメータ：𝜃
　→何度も投げて観測した「表・裏のデータ」に対して、「このデータが起こりやすいのは、どんな𝜃か？」

例：サイコロ投げ
　モデル：1〜6が出る確率が（$\theta_1~\theta_6$）
　パラメータ：各目の確率（合計 = 1）
　→「この出目の記録に一番合うような確率の組み合わせ（パラメータ）は？」

※コイン投げは少し特殊です。データ2種類しかないので、一つのモデルが𝜃でしたら、もう一つは(1-𝜃)になります
:::

- 数式：$L(\theta) = P(x_1 \mid \theta) \times P(x_2 \mid \theta) \times \cdots \times P(x_n \mid \theta) = \prod_{i=1}^{n} P(x_i \mid \theta)$
    - $P(x_i \mid \theta)$は、パラメータθを持つモデルにおいて、データ$x_i$が生成される確率を表します
    - $\prod_{i=1}^{n}$はすべてのデータについての積を表します

#### 例：コイン投げ
> 表3回、裏2回のデータを例で計算します

表の確率：𝜃
裏の確率：1-𝜃

$$
\begin{aligned}
L(\theta)&=_5C_3 \theta^3 \times (1-\theta)^2\\
&=10\theta^3(1-\theta)^2
&=10(\theta^5-20\theta^4+\theta^3)
\end{aligned}
$$
:::message
この$L(\theta)$の結果は二項分布そのものです
:::

:::message
もしデータは順番を指定する場合（例えば、表・裏・表・表・裏）、尤度関数は
$L(\theta) = \theta \cdot (1 - \theta) \cdot \theta \cdot \theta \cdot (1 - \theta) = \theta^3 (1 - \theta)^2$
です。
ただ、統計推定（とくに最尤推定）では、順序を問わない形が一般的です
:::


#### 例：サイコロ投げ
> サイコロを10回投げ、出た目は1,2,3,3,5,2,6,4,3,2です

各サイコロ目の出る確率は $\theta = (\theta_1,\theta_2,\theta_3,\theta_4,\theta_5,\theta_6)$
各サイコロ目(1~6)の出る回数は(1,3,3,1,1,1)


$L(\theta) = \theta_1 \cdot (\theta_2)^3  \cdot (\theta_3)^3 \cdot \theta_4 \cdot \theta_5 \cdot \theta_6$


### 最尤推定（Maximum Likelihood Estimation: MLE）
与えられたデータに対して、最も尤度が大きくなるようなモデルのパラメータを推定する方法
#### 最尤推定量
尤度が最も大きくなるようなモデルのパラメータのことです。言い換えると、確率が最大のものです


### 対数尤度関数
尤度自体はn個の掛け算になり、計算上不便なので、自然対数をとったものです
対数尤度関数：$l(\theta)=logL(\theta)$、𝜃を求める際に、$\frac{\partial}{\partial \theta}l(\theta)$を解く事が出来れば、θを求める事が出来ます

:::message
最大値を求める時に、対数尤度関数が0になったときに、L(𝜃)が最大になる
$\frac{\partial}{\partial \theta}l(\theta)=0$
:::

#### 負の対数尤度関数（Negative Log-Likelihood Function）
対数尤度に負号をつけた関数です
最尤推定では、対数尤度（log-likelihood）を「最大化」するが、
機械学習では、多くの最適化アルゴリズム（勾配降下法など）は最小化を行うため、
「最大化」問題を「最小化」問題に変換します

$NLL(\theta)=-l(\theta)=-logL(\theta)$

### 最尤推定の計算例
下の例で最尤推定量を計算します

> コインを5回投げ、表3回、裏2回がでる

尤度関数は$L(\theta)=10\theta^3(1-\theta)^2$

対数尤度関数を変換します


$l(\theta)=logL(\theta)=0+3log(\theta)+2log(1-\theta)$


微分します
$\frac{\partial}{\partial \theta}l(\theta)=\frac{3}{\theta}+\frac{2}{1-\theta}\cdot \frac{\partial 1-\theta}{\partial \theta}=\frac{3}{\theta}+\frac{2}{1-\theta}\cdot (-1)=\frac{3}{\theta}-\frac{2}{1-\theta}$

最尤推定量を求めます
$\frac{3}{\theta}-\frac{2}{1-\theta}=0$
$3(1-\theta)=2\theta$
$\theta=0.6$ 


:::message
前で話しました順番の問題ですが、仮に今回は順序が決まったデータ（例えば、表・裏・表・表・裏）であれば、
尤度関数は$L(\theta) =  \theta^3 (1 - \theta)^2$ですが、最尤推定量は変わらないです
:::


### ベイズ則
条件付き確率を使って、逆の条件付き確率を求めます

$P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}$

- $P(A \mid B)$：データB(結果)が与えられたときの、そのデータ(結果)に対する原因の推定Aが正しい確率。 (事後確率と呼ばれる。)
- $P(B \mid A)$：原因の推定Aが正しいとしたときの、データB(結果)の確率。
- $P(A)$：データ(結果)に関係なく、原因の推定Aが正しい確率。 (推定Aの事前確率と呼ばれる。)
- $P(B)$：原因の推定に関係ない、データB(結果)の確率。 (データBの事前確率と呼ばれる。)

:::message
条件付き確率
$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$

乗法定理
$P(A \cap B) = P(B) \cdot P(A \mid B)=P(A) \cdot P(B \mid A)$

:::


#### 例
> データB:検査結果は陽性
> 推定A：病気だった  


- $P(病気 \mid 陽性)$：検査結果が陽性の時、病気だったの確率
- $P(陽性 \mid 病気)$：病気である人が検査で陽性になる確率
- $P(病気)$：ある人が病気である確率　
- $P(陽性)$：検査を受けて、陽性になる確率



### ベイズ推定
ベイズ則を用いて、未知のパラメータの分布（事後分布）を推定する方法です

#### 例：検査結果から病気の確率を推定する
> ある病気の有病率（事前確率）：1%
> 検査の精度
> 　真陽性率（病気がかかった人に対して、検査結果が陽性になった確率）：0.99
> 　偽陽性率（健康な人に対して、検査結果が陽性になった確率）：0.05

1. 全体の陽性確率を計算

$$
\begin{aligned}
P(陽性)&=P(陽性 \mid 病気)\cdot P(病気)+P(陽性 \mid 健康)\cdot P(健康)
\\&=0.99\cdot 0.01 +0.05\cdot 0.99
\\&=0.0594
\end{aligned}
$$

2. ベイズ則に代入

$$
\begin{aligned}
P(病気 \mid 陽性) &= \frac{P(陽性 \mid 病気) \cdot P(病気)}{P(陽性)}
\\&=\frac{0.99 \cdot 0.01}{0.0594}
\\&\approx 0.1667
\end{aligned}
$$

**検査で陽性でも、実際に病気の確率は約16.7%しかない**


### MAP推定（Maximum A Posteriori estimation）
- 最尤事後推定とも呼ばれる
- 定義：ベイズ則に基づいて、事後確率が最大となるパラメータの推定方法です
- MAP推定はベイズ則を使って1つのベストな解を出す手法です

$\hat{\theta}_{\mathrm{MAP}}=\argmax_{\theta}P(\theta |D)$  →事後確率が最大となるパラメータを求めます

ベイズ則より

$P(\theta|D)=\frac{P(D | \theta) P(\theta)}{P(D)}$

よってMAPは次の式を最大化することと等価します

$\hat{\theta}_{\mathrm{MAP}}=\argmax_{\theta}P(D|\theta) P(\theta)$


:::message
$\argmax_{\theta}f(\theta)$：ある関数が最大になるときの θ の値を表します

なので、最尤推定量(MLE)も下記の通りになります

$\hat{\theta}_{\mathrm{MLE}}=\argmax_{\theta}P(D|\theta)$
:::


### ナイーブベイズ（Naive Bayes）
ナイーブベイズは特徴量$x_i$はカテゴリに条件づければ、**互いに独立である**と仮定するベイズ則です
- 特徴量：データの性質や情報を数値や記号で表現したものです。データの性質や情報をどのカテゴリに属するかの判断情報です

ベイズ則より

$$
P(y|x_1,\cdots,x_n)=\frac{P(y)P(x_1,x_2,\cdots,x_n|y)}{P(x_1,x_2,\cdots,x_n)}

$$

- y：分類したいカテゴリ
- $x_1,x_2,\cdots , x_n$：カテゴリ分類に使う要素
- $P(y|x_1,\cdots,x_n)$：$x_1,x_2,\cdots,x_n$が含まれる時、yである確率
- P(y)：全体のうち、yである確率。（=サンプル全部のうち、分類がyの割合）

ここで、**$x_1,x_2,\cdots , x_n$たちが互いの確率に影響を与えない（独立である）と仮定します**

この時、$x_i$以外が発生しているとき、$x_i$が発生する確率は

$$
P(x_i|y,x_1,\cdots,x_{n+1},\cdots,x_n)
$$


$x_i$の発生率は他のx影響を受けないため、

$$
P(x_i|y,x_1,\cdots,x_{n+1},\cdots,x_n)=P(x_i|y)
$$

となると、ナイーブベイズは

$$
P(y|x_1,\cdots,x_n)=\frac{P(y)P(x_1|y)\cdots P(x_n|y)}{P(x_1,x_2,\cdots,x_n)}=\frac{P(y)\displaystyle\prod_{i=1}^nP(x_i|y)}{P(x_1,x_2,\cdots,x_n)}
$$

#### 例：病気の診断
> ある病気（インフルエンザ）の有無を、3つの症状の有無から予測します

■症状（特徴量）：
- $x_1$：咳があるか
- $x_2$：熱があるか
- $x_3$：頭痛があるか

■yは病気の有無（カテゴリ）：
- y=あり
- y=なし


■観察データ：
| データ | 病気あり | 咳  | 熱  | 頭痛 |
|--------|-----------|-----|-----|-------|
| 1      | はい      | ○   | ○   | ○     |
| 2      | はい      | ○   | ○   | ×     |
| 3      | いいえ    | ×   | ○   | ○     |
| 4      | いいえ    | ×   | ×   | ×     |

新しい患者の症状
- 「咳あり・熱あり・頭痛なし」の患者が来たとき、この人が病気かどうかを推定します

----

■事前確率
P(病気あり)：1/2
P(病気なし)：1/2


■新しい患者の症状と観察データより、下記条件付き確率を求めます
P(咳あり|病気あり)=2/2=1
P(熱あり|病気あり)=2/2=1
P(頭痛なし|病気あり)=1/2
P(咳あり|病気なし)=0/2=0
P(熱あり|病気なし)=1/2
P(頭痛なし|病気なし)=1/2

■ナイーブベイズによる推定
- 病気ありのスコア：
$P(病気あり)\cdot P(咳あり|病気あり)\cdot P(熱あり|病気あり)\cdot P(頭痛なし|病気あり)=\frac{1}{2}\cdot 1 \cdot 1 \cdot \frac{1}{2}=\frac{1}{4}$

- 病気なしのスコア：
$P(病気なし)\cdot P(咳あり|病気なし)\cdot P(熱あり|病気なし)\cdot P(頭痛なし|病気なし)=\frac{1}{2}\cdot 0 \cdot \frac{1}{2} \cdot \frac{1}{2}=0$

■結論
病気ありと判定される


### 確率と尤度の違い
- 確率：「事前に知っている情報」（パラメータ）に基づいて、ある結果がどれだけ起こりやすいかを示します
- 尤度：ある結果が観測された場合に、その結果の尤もらしさ（生成されやすさ、ありやすさなど）を推測します
    - このデータが起こりやすいのは、どんなパラメータ（確率で表現する場合あり）か？
    - データから分布の前提条件を推測します
    - 尤度が大きければ、事後確率も大きくなります（事前が一定なら）

#### 例：コイン投げ
> 公正なコインを5回投げ、表3回、裏2回の結果になります

- 確率：「公正なコイン」を5回投げ、表3回、裏2回の結果の発生しやすさ
    - 発生の確率は5/16
- 尤度：表3回、裏2回の結果に対して、このコインはどのようなコインですか(公正なコインであることは知りません)
    - 計算の結果は、このコインは表が出る確率が60%のコインが尤もらしい（可能性が一番高い）です


#### ベイズ則の例
> 検査の結果は陽性だった

尤度：陽性の結果に対して、この人は本当に病気がかかった確率（病気なら検査結果が陽性になった）です（$P(陽性 \mid 病気)$）

:::message
ベイズ則は、「事前確率 × 尤度」で事後確率を求めるルール
:::



## 関係整理

### 最尤推定とベイズ推定の違い
- 最尤推定は得られたデータのみで推定するので信頼性が高いが、データ数が少ないとランダム性に依存してしまいます
- ベイズ推定は事前分布を生かして推測するので、少ないデータ数でもある程度適切な値を得ることができるが、事前情報の信頼が低いときは良い値が出ません
- 最尤推定が推定量の算出に事前情報を使わないのに対し、ベイズ推定は推定量の算出に事前情報を使います

### MAP推定と最尤推定量
- MAP推定：**ベイズ則**に基づいて、**事後確率が最大となるパラメータの推定**方法です
- 最尤推定量：**尤度**が**最も大きくなる**ようなモデルのパラメータのことです

### MAP推定とベイズ推定
- MAP推定：最も確からしいパラメータ値（1点）を求めます
- ベイズ推定：パラメータの分布そのもの（事後分布）を求めます

