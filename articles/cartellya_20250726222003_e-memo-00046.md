---
title: "BERT"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- BERTモデルと仕組みを勉強する

## キーワード
BERT, ファインチューニング, 事前学習, Masked Language Modeling,
Next Sentence Prediction, Positional Embedding, Segment Embeddings

## 学習内容
### BERT(Bidirectional Encoder Representations from Transformers)とは
- Googleが2018年に発表した自然言語処理（NLP）のための事前学習済みモデル
- アーキテクチャはtransformer

#### 特徴
| 特徴                      | 説明                                     |
| ----------------------- | -------------------------------------- |
| 双方向性（Bidirectional） | 文を左から右だけでなく、左右の文脈を同時に考慮して理解する      |
| 事前学習（Pre-training）  | 大量のテキストで事前に学習させ、汎用的な知識を獲得 |
| ファインチューニング可能        | さまざまなNLPタスク（質問応答、文分類など）に再調整できる         |


#### ファインチューニング(fine-tuning)
- 既に学習されたモデルを、特定のタスクに合わせて追加で学習させること
- 言い換えると、すでに頭のいいモデルに、目的に合わせた最後の仕上げの勉強をさせる
- 学習の流れ
    - 事前学習済みモデルを用意
    - タスク用のデータを用意
    - モデルの最後の層（出力層）を変更
    - 少量の学習データで再学習




### 事前学習(pre-training)
- モデルに対して大量の一般的なデータを使って、幅広い知識やパターンを学ばせるプロセスのこと
- 事前学習される理由
    - 大量の教師ありデータが必要
    - 訓練に時間もコストもかかる
    - 一般的な知識がないため、応用がきかない


#### 事前学習 → ファインチューニングの流れ
事前学習：汎用的な知識を学ぶ（Wikipedia、Booksなどで）
ファインチューニング：特定のタスクに合わせて微調整（スパム分類、質問応答など）
この2段階構造が、現在の多くの自然言語処理モデルで採用されている方式である



### BERTの事前学習

#### Masked Language Modeling（MLM）
- 入力文の中の単語をランダムに[MASK]にして、元の単語を予測させる
    - 例：私は[MASK]が好きです。 → 寿司
- 前後の文脈を考慮する双方向モデルで学習できるようになった

#### Next Sentence Prediction（NSP）
- 2つの文を与えて「2文目は1文目の次に実際に来る文か？」を予測する
    - をYes/Noの2クラスで予測する
- 例：
    - 文A: 「私は本を読んだ」
    - 文B: 「その後、寝た」 → ✅（正しいペア）
    - 文B: 「パンダは中国にいます」 → ❌（関係ない）
- 文と文の関係性や論理的なつながりを学習する

### Positional Embedding(位置埋め込み)
- 単語やトークンの「並び順（位置）」の情報をモデルに与えるための手法である
- Transformerは順番に処理するではないので、単語の並び順がわからない。なので、位置情報を数値で教えてあげる必要がある

```
文:         私     は     寿司     が     好き
位置:       0      1      2        3      4
Word Emb:   W0     W1     W2       W3     W4
Pos Emb:    P0     P1     P2       P3     P4
入力 =      W0+P0  W1+P1  W2+P2    W3+P3  W4+P4
```

### Segment Embeddings（セグメント埋め込み）
- 文の区切り（どの単語がどの文に属しているか）をモデルに伝えるためのベクトルである
    - 例えば、文Aと文Bが連続しているかどうかのタスクがあって、モデルは文Aと文Bがどこで区切られているかを知らないと、うまく学習できない

#### タスクの例
> 文Aと文Bが連続しているかどうか

```
文A：私は図書館に行きました。  
文B：本を3冊借りました。
→ ラベル：IsNext（続いている）
```

```
文A：私は図書館に行きました。  
文B：犬が庭で吠えていました。
→ ラベル：NotNext（続いていない）
```

> 文Aと文Bの関係を学ぶ

```
文A（質問）：日本の首都はどこですか？  
文B（文章）：日本の首都は東京であり、経済と政治の中心地です。

→ 文Bが質問に関係しているか？（分類タスク）
→ 文Bの中から「東京」という答えを抽出する（抽出型質問応答）
```


### 仕組み
各単語に対して、
- 文Aに属しているなら：セグメントID = 0
- 文Bに属しているなら：セグメントID = 1
というベクトル（Segment Embedding）を足し合わせて使う

### 例
> 文A: 「私は学生です」
> 文B: 「東京に住んでいます」

```
[CLS] 私は学生です [SEP] 東京に住んでいます [SEP]
```

| トークン   | Word Embedding | Positional Emb | Segment Embedding |
| ------ | -------------- | -------------- | ----------------- |
| \[CLS] | E₁             | P₁             | 0                 |
| 私      | E₂             | P₂             | 0                 |
| は      | E₃             | P₃             | 0                 |
| 学生     | E₄             | P₄             | 0                 |
| です     | E₅             | P₅             | 0                 |
| \[SEP] | E₆             | P₆             | 0                 |
| 東京     | E₇             | P₇             | 1                 |
| に      | E₈             | P₈             | 1                 |
| 住んで    | E₉             | P₉             | 1                 |
| います    | E₁₀            | P₁₀            | 1                 |
| \[SEP] | E₁₁            | P₁₁            | 1                 |
