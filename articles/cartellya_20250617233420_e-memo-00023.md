---
title: "パラメータの調整"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 最適なアルゴリズムを使って、パラメータ（重みとバイアス）を調整する


#### モデル訓練のステップ
1. アーキテクチャの設計
2. 入力層設計
3. 中間層設計
4. 出力層設計
5. 誤差計算
6. モデルに誤差を反映する
7. **重みの更新の設定をする**
8. 最適なモデルを手に入れる

## キーワード
学習率, 鞍点, 最急降下法, 確率的勾配降下法, モメンタム,
ネステロフのモメンタム, AdaGrad, RMSProp, Adam, 早期終了

## 学習内容
### 最適なパラメータの調整
勾配の計算と重みの更新を繰り返すことで、誤差の小さくなる重みパラメータを見つける繰り返し計算を行います。
最適なパラメータをなるべく早く見つけるための最適化アルゴリズムが研究される

### 学習率
1回の学習で、どれだけ学習すべきか、どれだけパラメータを更新するか、というものをきめること

### 鞍点(Saddle Point)
最小値でも最大値でもないが、勾配がゼロになる点であること

### 最急降下法(Gradient Descent)
- 最も基本的で広く使われる最適化アルゴリズム
- 関数の勾配を使って、「いまいる点から最も急に下がる方向」へ少しずつ移動することで、最小値に近づいていく

:::message
イメージとしては、
山を下る登山者が目の前の一番傾斜がきつい方向に一歩進む
一歩進んだらまた傾きを計算し、また一番下る方向へ進む
これを繰り返して、山のふもと（最小値）を探す

:::

### 確率的勾配降下法(Stochastic Gradient Descent, SGD)
- 最急降下法の改良版
- 通常の勾配降下法はすべての訓練データを使って勾配を計算するが、SGDは1つのデータ（または小さなバッチ）だけを使って勾配を計算・更新する

$$

\theta := \theta - \alpha \nabla L(\theta)

$$

$\theta$：パラメータ
α：学習率
$\nabla L(\theta)$：勾配

#### 問題点
- 勾配が小さい方向に学習が進みづらいため、学習時間がかかる

:::message
勾配降下法は勾配が0になるところを探すが、それが最小値に限らない。また、勾配が0になると、学習が進まない問題がある。これは勾配法の特徴である
:::

### モメンタム(Momentum)
- SGDから改良する
- 過去の勾配の累積的影響を考慮して更新する（SGDは過去の勾配を考慮しない）

$$
v_t = \gamma v_{t-1} + \alpha \nabla L(\theta_t)
$$
$$
\theta_{t+1} = \theta_t - v_t
$$

| 記号                   | 意味                   |
| -------------------- | -------------------- |
| $\theta_t$           | tステップ目のパラメータ         |
| $v_t$                | モメンタムのベクトル（速度）       |
| $\alpha$               | 学習率   |
| $\gamma$             | モメンタム係数（慣性の強さ、0.9や0.99などが一般的） |
| $\nabla L(\theta_t)$ | 勾配          |

#### メリット
- 学習の加速（谷の方向に勢いをつけて素早く収束）
- 鞍点の脱出にも有効（停滞せずに抜けやすくなる）
- 振動の抑制（ノイズが影響しにくい）

### ネステロフのモメンタム(Nesterov Accelerated Gradient, NAG)
- モメンタム法の改良版、より先を読んだパラメータ更新が可能になる
- 今の勾配ではなく、少し先に進んだ場所の勾配を使って更新する

$$
v_t = \gamma v_{t-1} + \alpha \nabla_\theta L(\theta_t - \gamma v_{t-1})
$$
$$
\theta_{t+1} = \theta_t - v_t
$$

### AdaGrad (Adaptive Gradient Algorithm)
- 勾配降下法の一種で、各パラメータごとに個別の学習率を自動調整する最適化アルゴリズム
- よく更新されるパラメータの学習率を小さくし、あまり更新されないパラメータは学習率を大きく保つこと。学習率に応じてそれぞれの重みを補正する。


#### 数式
$$
G_t = G_{t-1} + g_t^2
$$
$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} g_t
$$
| 記号         | 意味                              |
| ---------- | ------------------------------- |
| $g_t$      | 勾配（$\nabla_\theta L(\theta_t)$） |
| $G_t$      | 過去の勾配の **2 乗和**（要素ごとの累積）        |
| $\eta$     | 初期学習率                           |
| $\epsilon$ | 0除算防止のための微小値（例：$10^{-8}$）       |

#### 問題点
- $G_t$は徐々に大きくなっていくため、時間が経つと学習率がどんどん小さくなり、学習が進まなくなる可能性がある

### RMSProp(Root Mean Square Propagation)
- 勾配降下法の一種で、AdaGradの欠点（学習率が小さくなりすぎる問題）を解決するために改良された最適化手法
- 学習率が極端に小さくなる問題を解決するために、勾配の「移動平均」（累積二乗和ではなく） を使って、分母の大きくなりすぎを防ぐ

#### 数式
$$
E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g_t^2
$$
$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} g_t
$$

| 記号         | 意味                         |
| ---------- | -------------------------- |
| $g_t$      | 時刻 $t$ における勾配              |
| $E[g^2]_t$ | 勾配の2乗の**指数移動平均**（過去の情報を反映） |
| $\gamma$   | 減衰率（例：0.9）                 |
| $\alpha$     | 学習率（例：0.001）               |
| $\epsilon$ | 0除算防止のための微小値（例：$10^{-8}$）  |


### Adam(Adaptive Momentum)
- モメンタムとRMSPropを組み合わせたもので、現状一番評価されている手法

#### 数式
**1次モーメント（モメンタム）の更新**

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$

**2次モーメント（RMSProp的要素）の更新**

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

**バイアス補正**

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

**パラメータの更新**

$$
\theta_{t+1} = \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

| 記号         | 意味                     |
| ---------- | ---------------------- |
| $g_t$      | 時刻 $t$ の勾配             |
| $m_t$      | 勾配の移動平均（1次モーメント）       |
| $v_t$      | 勾配の2乗の移動平均（2次モーメント）    |
| $\alpha$     | 学習率（通常 0.001）          |
| $\beta_1$  | モメンタム係数（通常 0.9）        |
| $\beta_2$  | RMSProp係数（通常 0.999）    |
| $\epsilon$ | 0除算防止の微小値（例：$10^{-8}$） |

#### 特徴
- モメンタムのように過去の勾配（1次モーメント）を使って「方向」を滑らかにする
- RMSPropのように勾配の2乗の平均（2次モーメント）を使って学習率を調整する
- バイアス補正が入っているので、初期ステップの不安定さも補正

### 早期終了(earlystopping)
- モデルの学習を途中で止めて、過学習を防ぐテクニック
- 学習反復回数(epoch)ごとに訓練誤差とバリデーション誤差を記録し、誤差が途中から大きくなる時点で学習を止める手法