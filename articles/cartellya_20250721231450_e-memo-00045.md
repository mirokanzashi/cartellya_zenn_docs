---
title: "Word2vec"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- Word2vecとWord2vecの学習モデルを勉強する

## キーワード
Word2vec, n-gram, CBOW, skip-gram ,ネガティブサンプリング

## 学習内容

### Word2vec
- 単語を意味的な関係を保ったまま「ベクトル（数値）」に変換するWord Embedding技術の一つ

| 長所                | 短所                        |
| ----------------- | ------------------------- |
| 意味的に似た単語を近くに配置できる | 文脈（前後の意味）には対応していない        |
| 計算が高速（学習も軽量）      | 単語ごとに固定ベクトル（文脈で変化しない）     |
| 多様な言語に対応可能        | 未知語（OOV）に弱い |


#### 考え方
- 「意味が似ている単語は、似た文脈で使われる」
- たとえば：
    - 「dog」と「puppy」
    - 「king」と「queen」
- この共起情報を使って、単語を連続値ベクトルとして学習する

#### Word2Vecのモデル
| モデル名                              | 説明              |
| --------------------------------- | --------------- |
| CBOW（Continuous Bag of Words） | 周囲の単語から中心の単語を予測 |
| skip-gram                     | 中心の単語から周囲の単語を予測 |

###  n-gram
- テキスト中の「連続したn個の単語や文字の列」を取り出す方法。自然言語処理において、単語の順序や文脈を考慮するための基本的なアイデアである
- 言語モデルでは次の単語予測などで使う

例：
> 私はりんごを食べた

1-gram（unigram） → 単語単位
　→ ["私", "は", "りんご", "を", "食べた"]

2-gram（bigram）
　→ [("私", "は"), ("は", "りんご"), ("りんご", "を"), ("を", "食べた")]

3-gram（trigram）
　→ [("私", "は", "りんご"), ("は", "りんご", "を"), ("りんご", "を", "食べた")]

#### CBOW model
- 文脈（周囲の単語）から中心の単語を予測するモデルである
#### 例
> 中心語＝"playing"、ウィンドウサイズ=2

文脈（周囲の単語）：`["I", "love", "the", "guitar"]`
予測する単語（中心語）：`playing`
→ モデルは「I, love, the, guitar」を見てplayingを予測できるように学習する

:::message
ウィンドウサイズ（window size）：単語の前後何語を「文脈」として見るかを決めるパラメータである
例：私はギターを弾くのが好き
- 中心語：「弾く」
- ウィンドウサイズ = 1：文脈単語は「を」「の」
- ウィンドウサイズ = 2：文脈単語は「ギター」「を」「の」「が」
- ウィンドウサイズ = 3：文脈単語は「は」「ギター」「を」「の」「が」「好き」

:::

#### 数式
1. 各文脈単語 → one-hot → 埋め込みベクトルへ

2. 文脈ベクトルを平均
各文脈単語のone-hotベクトル$x_i$を使って、ベクトル平均を取る

$$
h = \frac{1}{2c} \sum_{\substack{-c \leq j \leq c \\ j \neq 0}} W^\top x_{t+j}
$$

3. 出力ベクトルと掛けてsoftmaxで確率(中心語の予測確率)
得られた文脈ベクトルhを使って、中心語$w_t$が出る確率を予測

$$
P(w_t \mid \text{context}) = \frac{\exp({v'_{w_t}}^\top h)}{\sum_{i=1}^V \exp({v'_i}^\top h)}
$$
​
4. 実際の中心語との誤差で損失を計算・学習
学習では、この予測確率と実際の中心語との誤差（交差エントロピー）を最小化

$$
\mathcal{L} = - \sum_{t=1}^{T} \log P(w_t \mid \text{context})
$$
 

$V$：語彙サイズ（単語の種類数）
$N$：単語ベクトルの次元数
$w_t$：時刻 $t$ の中心単語（予測対象）
文脈単語の集合：$\{ w_{t-c}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+c} \}$ （$c$ はウィンドウサイズ）
$x_i \in \mathbb{R}^V$：単語 $w_i$ のワンホットベクトル
$W \in \mathbb{R}^{V \times N}$：入力側の単語埋め込み行列
$W' \in \mathbb{R}^{N \times V}$：出力側の重み行列（予測用）
$v_i = W^\top x_i \in \mathbb{R}^N$：単語 $w_i$ の埋め込みベクトル
$v'_i$：出力側で使う単語 $w_i$ のベクトル（$W'$ の列）


### skip-gram
- 中心の単語 → 周囲の単語（文脈）を予測する

#### 数式

- 入力単語が与えられた時に𝑗番目の文脈語が出力される確率

$$
P(w_{t+j} \mid w_t) = \frac{\exp\left({v'_{w_{t+j}}}^\top v_{w_t}\right)}{\sum_{i=1}^V \exp\left({v'_i}^\top v_{w_t}\right)}
$$

$v_{w_t}$：中心語のベクトル
$v'_{w_{t+j}}$：文脈語の出力ベクトル
$V$：語彙サイズ

- 1つの中心語から複数の文脈語を予測するため、各ペアに対して損失(損失関数)

$$
\mathcal{L} = - \sum_{t=1}^{T} \sum_{\substack{-c \leq j \leq c \\ j \neq 0}} \log P(w_{t+j} \mid w_t)

$$

#### まとめ
| 項目   | 内容                      |
| ---- | ----------------------- |
| 目的   | 中心語から周囲の単語を予測する     |
| 特徴   | レア語に強く、高精度なベクトルを学習しやすい  |
| 弱点   | CBOWより計算コストが高い（学習時間が長め） |


### CBOW vs skip-gram 比較

| 比較項目     | CBOW        | skip-gram             |
| -------- | ----------- | --------------------- |
| 予測の方向    | 周囲の単語 → 中心語 | 中心語 → 周囲の単語           |
| 学習速度     | 速い（少ない出力）   | 遅い（出力が複数ある）           |
| レア単語への強さ | 弱い          | 強い（良いベクトルを学習しやすい） |
| 小規模データ   | 不向き         | 向いている                 |

### ネガティブサンプリング(Negative Sampling)
- Word2Vec のようなモデルを効率的に学習するための手法で、「正しい組み合わせ（例：catとpet）だけでなく、あえて関係のないペアも学習させる」ことによって、単語ベクトルを効果的に学習する

例えば、skip-gramでは、
- 中心単語 → 周囲の単語（= 正例）を予測
- でも、語彙が多すぎて全単語と比較すると非効率

ネガティブサンプリングは、
- 中心単語と実際の周囲単語（正例）はそのまま学習
- 同時に、ランダムに選んだ関係ない単語（負例）を学習に使う

| 中心単語  | 文脈単語    | ラベル   |
| ----- | ------- | ----- |
| dog | bark  | 1（正例） |
| dog | table | 0（負例） |
| dog | car   | 0（負例） |

#### メリット
| 項目       | 内容                     |
| -------- | ---------------------- |
| 高速       | 全語彙を使うsoftmaxに比べて大幅に高速 |
| メモリ効率が良い | 一部の負例だけでOK             |
| 学習が安定する  | 頻出語ばかりに引っ張られにくい        |
