---
title: "性能指標"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 学習済みモデルの評価の指標を勉強します(回帰の評価指標以外)


## キーワード
混同行列,正解率,適合率,再現率,F値,
ROCカーブ,AUCスコア,micro平均,macro平均,
IoU


## 学習内容
### 混同行列（Confusion Matrix）
|                 | **予測：Positive**    | **予測：Negative**    |
| --------------- | ------------------ | ------------------ |
| **実際：Positive** | TP（True Positive）  | FN（False Negative） |
| **実際：Negative** | FP（False Positive） | TN（True Negative）  |


- TP（真陽性）：真の結果と予測結果の両方が陽性
- TN（真陰性）：真の結果と予測結果の両方が陰性
- FP（偽陽性）：真の結果が陰性であり、予測結果が陽性（誤検知）
- FN（偽陰性）：真の結果が陽性であり、予測結果が陰性（見逃し）


### 正解率(Accuracy)
- 全体のうち、正しく予測できたものの割合
- 偏ったデータの場合信頼性が低いです。

:::message
例えば、99個がクラス0、1個がクラス1のデータ→全部「クラス0」と予測しても accuracy = 99%になります
:::

- 数式：$精度=\frac{正解した数}{全部データ数}=\frac{TP+TN}{TP+TN+FP+FN}$


### 適合率(Precision)
- 陽性と予測されたもののうち、真の結果が陽性であるものの割合
- 数式：$適合率=\frac{TP}{TP+FP}$

### 再現率(Recall)
- 陽性と予測されたもののうち、真の結果が陽性であるものの割合
- 真陽性率(True Positive Rate, TPR)と同意
- 数式：$適合率=\frac{TP}{TP+FN}$


### 真陰性率(True Negative Rate, TNR)
- 真の結果が陰性のものから陰性と予測されたものの割合
- 数式：$真陰性率=\frac{TN}{TN+FP}$

### 偽陽性率(False Positive Rate, FPR)
- 真の結果が陰性のものを陽性と誤って予測した割合
- 数式：$偽陽性率=\frac{FP}{TN+FP}$

### 偽陰性率(False Negative Rate, FNR)
- 真の結果が陽性のものを陰性と誤って予測した割合
- 数式：$偽陰性率=\frac{FN}{FN+TP}$

### F値(F-Score)
- 適合率と再現率のバランスを考慮した調和平均です
- 数式：$F値=2\times \frac{適合率\times 再現率}{適合率+再現率}$


### ROCカーブ(Receiver Operating Characteristic Curve)とAUC(area under the curve)スコア

![](/images/e-memo-00012_01.png)
*出典：
Evidently AI Team, How to explain the ROC curve and ROC AUC score?(2025), https://www.evidentlyai.com/classification-metrics/explain-roc-curve/*

- X軸：偽陽性率
- Y軸：真陽性率（再現率）

### ROCカーブの見方
- 左上に近いほど優れたモデル
- 対角線はランダム予測と同じ性能
- ROC曲線が上に膨らんでいるほど良い

### AUCスコア
- ROCカーブの下の面積を数値化したものです
- 範囲は 0〜1
- 1.0 に近いほどモデルが優秀
- 0.5 は完全なランダム予測と同じ
- 正例と負例をランダムに1つずつ選んだとき、正例の方を高スコアで判定できる確率の意味です。



### AP(Average Precision)とmAP(mean Average Precision)
- APは物体検出や情報検索の精度を1つの指標で評価するための指標で、Precision-Recall（適合率-再現率）曲線の下の面積を意味します
    - X軸：再現率(Recall)
    - Y軸：適合率(Precision)
- mAPは複数クラスがある場合、各クラスごとのAPを平均したものです
    - 値の範囲は0~1で、1に近いほど検出の精度が高いと評価されます

![](/images/e-memo-00012_02.png)
*出典：
Evidently AI Team, Mean Average Precision (MAP) in ranking and recommendations(2025), https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map*

### micro平均
- 全クラスをまとめて計算する性能指標です
- 全てのクラスのTP, FP, FNを合算してから精度・再現率・F1スコアを計算する手法です
- 全体の正解率を反映して、クラス不均衡に敏感します
- 全体の性能を重視する場合に適しています

- micro適合率：$\text{Precision}_{\text{micro}} = \frac{\sum TP}{\sum (TP + FP)}$

- micro再現率：$\text{Recall}_{\text{micro}} = \frac{\sum TP}{\sum (TP + FN)}$

- microF値：$F_{\text{micro}} = \frac{2 \cdot \text{Precision}_{\text{micro}} \cdot \text{Recall}_{\text{micro}}}{\text{Precision}_{\text{micro}} + \text{Recall}_{\text{micro}}}$


### macro平均
- 各クラスの指標を個別に計算し、それらの平均を取る性能指標です
- 各クラスのPrecision/Recallを平均
- クラスごとのバランス評価です
- 各クラスの重要度が同等の場合やデータの少ないクラス重視する場合に適しています

- macro適合率：$\text{Precision}_{\text{macro}} = \frac{1}{C} \sum \text{Precision}$

- macro再現率：$\text{Recall}_{\text{macro}} = \frac{1}{C} \sum \text{Recall}$

- macroF値：$F_{\text{macro}} = \frac{1}{C} \sum F$

※C: クラス数

### IoU(Intersection over Union)
- 主に物体検出などで使われる評価指標で、予測領域と正解領域（Ground Truth）がどれくらい重なっているかを測るものです
- $\text{IoU} = \frac{|\text{A} \cap \text{B}|}{|\text{A} \cup \text{B}|}$
    - ∩：積集合
    - ∪：和集合
    - ∣⋅∣：面積（または要素数）


![](/images/e-memo-00012_03.jpg)
*出典：
Gaudenz Boesch(viso.ai) ,IoU Formula: The Intersection over Union (IoU) equals the Area of Intersection, divided by Area of Union, What is Intersection over Union (IoU)?(2024), https://viso.ai/computer-vision/intersection-over-union-iou/*