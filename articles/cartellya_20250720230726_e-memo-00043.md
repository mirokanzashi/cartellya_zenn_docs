---
title: "Vision Transformer"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- Vision Transformerを勉強する

## キーワード
Vision Transformer, CLSトークン, Position Embedding 

## 学習内容

### Vision Transformer
- 画像を「パッチ（小さな領域）」に分割し、それを「単語のように扱う」ことで、Transformerの入力として使う

### 処理の流れ
![](/images/e-memo-00043_01.png)
*出典：
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.*

1. 入力画像をパッチに分割する
    - TransformerはInputをシーケンスデータとして受け取る必要がある。そのため二次元である画像データをパッチごとに一次元のシーケンスデータに変換したうえで、線形射影する
2. パッチをベクトル化（埋め込み）：各パッチを1つのベクトル（embedding）に変換
3.位置情報を追加（Position Embedding）：パッチがどの位置にあるかを表すベクトルを加算
4. Transformer Encoderに入力：従来の言語モデルと同じく、Self-Attentionで全パッチ間の関係を学習
5. CLSトークンの出力を用いて分類：最初に追加した特殊なトークン（CLS）の最終出力を使って画像のクラスを判定

#### CLSトークン
- 入力の最初に追加される「特別なトークン」
- 最終的に、このトークンの出力ベクトルを分類タスクの特徴ベクトル（表現）として使う

#### Position Embedding 
- Transformerが「入力の順番や位置」を理解できるようにするための仕組み
- 入力の「順番・位置」の情報を埋め込みベクトル化

```
私はりんごを食べた。   → [CLS] 私 は りんご を 食べた
↓
入力 = [CLS] + patch1 + patch2 + ... + patchN
↓
各パッチ埋め込みに「位置ベクトル（Position Embedding）」を加算
↓
最終入力 = patch_embedding + position_embedding
```


### 特徴
- 畳み込みを行わない
- パッチ間の関連度を計算し計算量を削減
- スケーラビリティの高いアーキテクチャ：学習データが大量にあれば、CNNを超える性能を発揮