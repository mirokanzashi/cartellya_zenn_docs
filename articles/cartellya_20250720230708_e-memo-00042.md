---
title: "ResNet"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- ResNetとWideResNetを勉強する

## キーワード
ResNet, Skip connection, 残差接続, Residual Block,
ボトルネック構造, WideResNet

## 学習内容

### ResNet(Residual Networks：残差ネットワーク)とは
- 勾配消失問題や劣化問題によって学習が進まない問題をResidualblockという手法を使って解決し、	層を深くしても学習がうまくいくCNNの実現
- 通常、ニューラルネットワークの層を深くすれば性能が上がると考えられますが、ある程度の深さを超えると
    - 勾配消失・勾配爆発が起きる
    - 精度が劣化(degradation)する→学習が進まず、浅いモデルよりも性能が下がる
    - 劣化は勾配消失に関係なく、層を深くした際に精度が劣化してしまう
    - 残差学習(Residual Learning)で問題を解決する

#### ResNetのバリエーション

| モデル名       | 層の数  | 用途                   |
| ---------- | ---- | -------------------- |
| ResNet-18  | 18層  | 軽量モデル                |
| ResNet-34  | 34層  | 高速＋そこそこの精度           |
| ResNet-50  | 50層  | 一般的に使われる高精度モデル       |
| ResNet-101 | 101層 | より深いモデル              |
| ResNet-152 | 152層 | ImageNetトップモデル（超高精度） |


### 残差接続(Residual connection)
- 英語表記として、Skip connectionやShortcut connectionを書くことがある
- ResNetのベースとなっている、勾配消失問題を解決するための仕組み

#### 仕組み
- 通常のニューラルネットワークでは、データは層を1つ1つ順番に通っていきます（直列構造）。
- 残差接続では、ある層の出力を飛ばして、後の層に直接加えるという並列の経路を追加します。
- 変換が必要ない場合は𝐹(𝑥)の重みを0 にし、小さな変換が求められる場合は差分を学習しやすくなる。
- 通常の学習：入力xで出力がH(x)になる
- 残差接続で学習：$F(x) = H(x) - x   →   H(x) = F(x) + x$


![](/images/e-memo-00042_01.jpg)
*出典：
DeepAge, Residual Network(ResNet)の理解とチューニングのベストプラクティス(2016), https://deepage.net/deep_learning/2016/11/30/resnet.html*

- x: 入力（スキップされるデータ）
- F(x): 通常の変換（Conv + BN + ReLUなど）、学習すべき「残差」
- F(x) + x: 残差結合（Residual Connection）
- H(x): 学習したい最終出力


| 通常の学習                 | 残差接続で学習                       |
| --------------------- | -------------------------- |
| `H(x)` を1から学習する必要がある  | `x` に近いなら `F(x)` は「ほぼ0」で良い |
| ネットワークが重みを慎重に調整してH(x)≈xを作る必要がある | F(x)=H(x)-x=0を学べばいい（ゼロを出力するだけ） |
| 層が深くなると勾配が消えやすい       | `x` がショートカットでそのまま伝播される     |

### 残差ブロック(Residual Block)
- ResNetにおいて用いられる基本構造の単位で、「入力に対して、変換後の出力を加える（残差接続）」構造を含んだ小さなネットワークブロックである

#### 構造

```
       x
       │
       ├──────────────┐
       │              │
       ▼              ▼
   Conv (3x3)       （Identity）
     ↓
   BN
     ↓
   ReLU
     ↓
   Conv (3x3)
     ↓
   BN
     ↓
     + ←───── x（入力を足し算）
     ↓
   ReLU（出力）
```
#### 種類
- Basic Block（ResNet-18, 34で使用）
    - Conv (3x3) → BN → ReLU → Conv (3x3) → BN
    - 入力を加算 → ReLU
- Bottleneck Block（ResNet-50, 101, 152で使用）
    - Conv (1x1) → Conv (3x3) → Conv (1x1)
    - チャネル数を圧縮 → 処理 → 拡張（より効率的）
- 入力 x と F(x) の次元（チャネル数や空間サイズ）が異なると、そのまま加算できないので、1x1の畳み込み（Conv1x1）で次元を合わせる。
$ x → Conv1x1 → x'$
$y = F(x) + x'$

### ボトルネック(Bottleneck Block)構造
- ResNet-50 以上の深いモデルで使われる、より効率的な残差ブロックの構造である

#### 構造
- 中央の3x3 Convの前後で、チャネル数を一度減らしてから増やす構造
- 「通り道を一度狭くしてから広げる」ため、ボトルネックと呼ばれる

```
1x1 Conv（チャネル圧縮）  
→ 3x3 Conv（特徴抽出）  
→ 1x1 Conv（チャネル拡張）  
→ 残差接続（＋ x）

## 例
入力チャネル: 256
圧縮：1x1 Conv → 64
中央処理：3x3 Conv（64チャネル）
拡張：1x1 Conv → 256（元に戻す）
```

```
       x（入力）
        │
        ├────────────────────────────┐
        │                            │
        ▼                            ▼
    1x1 Conv（チャネル圧縮）        （Identity or 1x1 Conv）
        ↓
      BN + ReLU
        ↓
    3x3 Conv（特徴抽出）
        ↓
      BN + ReLU
        ↓
    1x1 Conv（チャネル拡張）
        ↓
         BN
        ↓
    ＋ ←─────── x（shortcut経路）
        ↓
      ReLU（出力）
```

#### 比較
| 項目      | Basic Block（ResNet-18/34） | Bottleneck Block（ResNet-50以降） |
| ------- | ------------------------- | ----------------------------- |
| 主な構造    | Conv(3x3) → Conv(3x3)     | 1x1 → 3x3 → 1x1               |
| チャネル数変化 | 変化なし              | 圧縮 → 処理 → 拡張       |
| 計算コスト   | 高め（浅い層向け）       | 低め（深い層でも使える）      |
| 学習安定性   | 良好             | より深くても安定            |


### WideResNet(Wide Residual Network)
- 深さ（Depth）ではなく、幅（Width）を広げることで性能を向上させた残差ネットワーク
- ResNet の改良版の一種
- 命名規則：WideResNet-28-10
    - 28：総レイヤー数（全体の深さ）
    - 10：widen factor（チャネル拡張率）

| 項目      | ResNet              | WideResNet        |
| ------- | ------------------- | ----------------- |
| アーキテクチャ | 非常に深い（50層〜1000層以上）  | 浅いが広い（16層, 28層程度） |
| チャネル数   | 通常（例: 64, 128, 256） | 2倍〜10倍以上に拡張されることも |
| 学習の難しさ  | 勾配消失・学習不安定の懸念       | 浅いので安定しやすい        |
| 計算コスト   | 高（深い）               | 高（幅が広い）が効率的に学習できる |
| 性能      | 高精度だが学習が困難な場合も      | 同等かそれ以上の精度を簡単に達成  |

#### 構造

```
通常の ResNet Block:
Conv(3x3, 64) → Conv(3x3, 64)

WideResNet (k=4):
Conv(3x3, 256) → Conv(3x3, 256)
```

- kはwiden factor（チャネル拡張率）と呼ばれる

#### メリット
- 深すぎると勾配消失や過学習が起こりやすい
- 幅を広げることで、特徴表現能力が大きく向上
- 並列計算がしやすく、GPUでの訓練効率が良い
- 残差接続のおかげで、広くしても学習が安定

### ボトルネットとWideResNetの違い
| 比較項目     | Bottleneck 構造              | Wide 構造                   |
| -------- | -------------------------- | ------------------------- |
| 主な目的     | 計算量・パラメータ削減（圧縮）      | 表現力強化（拡張）             |
| チャネル構造   | 1x1 で圧縮 → 3x3 処理 → 1x1 で拡張 | すべてのConvを幅広チャネル数に |
| 主な使われ方   | ResNet-50,101,152（深いモデル） | WideResNet（浅くても強いモデル）   |
| 精度向上の方法  | 深さ（Depth）を増やす              | 幅（Width）を増やす          |
| モデルの複雑さ  | 浅いが圧縮されたチャネル構成       | 浅くて広いチャネル構成       |
| 計算効率     | パラメータ削減重視（高効率）        | 並列性あり（GPUで高速）         |
| 残差接続への影響 | 出力チャネル数を元に戻す必要あり（Conv1x1）  | チャネルを揃えて加算（または調整）         |
