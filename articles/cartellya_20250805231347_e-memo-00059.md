---
title: "深層強化学習"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 強化学習と深層強化学習を勉強する

## キーワード
強化学習, 価値関数, 深層強化学習

## 学習内容

### 強化学習
- 「環境とやり取りしながら、報酬を最大にする行動戦略（方策）を学ぶ」学習手法

#### フレームワーク
```
状態 s_t → 行動 a_t → 報酬 r_t+1, 新状態 s_t+1
```

| 用語                   | 意味                  | 
| -------------------- | ------------------- | 
| 状態（State）$s$         | 現在の環境の情報（例：ゲーム画面）   |
| 行動（Action）$a$        | エージェントがとる選択（例：右に移動） |  
| 報酬（Reward）$r$        | 行動の良さを数値化（例：得点 +1）  |  
| 方策（Policy）$\pi(a \| s)$      | 状態に応じてどの行動を選ぶかの確率分布 |
| 価値関数（Value Function） | 状態や行動の「将来の報酬の期待値」   |        

#### 価値関数
- V関数・Q関数 をまとめた総称
- 「状態」や「行動」の良さ（将来的な報酬）を測る
- V関数 → 状態の良し悪しだけ知りたいとき
- Q関数 → 行動ごとの評価をしたいとき（方策選択に使う）
- 行動まで考えるならQ関数、状態だけならV関数

| 関数                           | 意味                                             | 入力             | 出力       | 目的            |
| ---------------------------- | ---------------------------------------------- | -------------- | -------- | ------------- |
| 状態価値関数（V関数） $V^\pi(s)$  | 状態 $s$ にいるとき、方策 $\pi$ に従って得られる期待累積報酬           | 状態 $s$         | 数値（期待報酬） | 状態の「良さ」を測る    |
| 行動価値関数（Q関数）$Q^\pi(s, a)$ | 状態 $s$ で行動 $a$ を取ったとき、以降方策 $\pi$ に従ったときの期待累積報酬 | 状態 $s$, 行動 $a$ | 数値（期待報酬） | 状態＋行動の「良さ」を測る |

### 深層強化学習(Deep Reinforcement Learning, Deep RL)
- 深層学習＋強化学習を組み合わせた手法
- ニューラルネットワークを使って、強化学習の価値関数や方策を近似する方法
- 従来の強化学習は状態・行動が少数・離散的な場合にうまくできるが、状態空間が連続・高次元（例：画像やセンサーデータ）になると限界がある
    - ニューラルネットワークで価値関数や方策を近似することで、大規模・複雑な環境にも対応する強化学習
- 欠点
    - 学習が不安定（発散しやすい）
    - 学習に非常に多くの試行が必要（データ効率が悪い）
    - 探索 vs 活用のバランスが難しい


| 構成                | 例                    | 説明                            |
| ----------------- | -------------------- | ----------------------------- |
| 価値関数をNNで近似    | DQN（Deep Q-Network）  | 状態を入力にしてQ値を出力するNNを学習          |
| 方策をNNで出力      | Policy Gradient, PPO | NNの出力が「行動確率」や「連続行動」になる        |
| Actor-Critic法 | A3C, A2C, DDPG       | 方策（Actor）と価値（Critic）を別々のNNで学習 |
