---
title: "出力層設計"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録","ニューラルネットワーク"]
published: true
---

## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 出力層で使用する活性化関数を勉強します

#### モデル訓練のステップ
1. アーキテクチャの設計
2. 入力層設計
3. 中間層設計
4. **出力層設計**
5. 誤差計算
6. モデルに誤差を反映する
7. 重みの更新の設定をする
8. 最適なモデルを手に入れる

## キーワード
線形ユニット, シグモイドユニット, ソフトマックスユニット, ソフトマックス関数

## 学習内容


### 出力ユニット
- ニューラルネットワークの出力層です。タスクによって以下の種類があります
    - 線形ユニット
    - シグモイドユニット
    - ソフトマックスユニット


### 線形ユニット
- ガウス出力分布が想定され、主に回帰問題で使用されます
- 活性化関数は恒等関数です

:::message
**恒等関数：** 入力した値と同じ値を常にそのまま返す関数です
$f(x)=x$
:::

### シグモイドユニット
- ベルヌーイ出力分布が想定され、主に2クラス分類問題で使用されます
- 活性化関数はシグモイド関数

$$
f(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}(1+e^{-x})}=\frac{e^{x}}{1+e^{x}}
$$

### ソフトマックスユニット
- マルチヌーイ出力分布が想定され、主に多クラス分類問題で使用されます
- 活性化関数はソフトマックス関数

#### ソフトマックス関数
- 出力の合計が1になるようにそれぞれの入力値を変換する関数です

通常のソフトマックス：

$$
y_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
$$

$y_1+y_2+...+y_n=1$

温度パラメータTを使ったソフトマックス：

$$
y_i = \frac{e^{x_i/T}}{\sum_{j=1}^{n} e^{x_j/T}}
$$

- T=1（通常）
- T<1：シャープになる（最大値により強く引っ張られる）
- T>1：なだらかになる（確率がより均等になる）
- T→0：ほぼ1-hotベクトルに近づく（最も高いスコアに集中）
- T→∞：一様分布に近づく（すべての選択肢が同じ確率）


### 活性化関数の比較
| 関数名     | 数式                             | 出力範囲                  | 主な用途    |
| ------- | ------------------------------ | --------------------- | ------- |
| 恒等関数    | $f(x) = x$                     | $-\infty \sim \infty$ | 回帰      |
| シグモイド   | $\frac{1}{1 + e^{-x}}$         | $0 \sim 1$            | 二値分類    |
| ReLU    | $\max(0, x)$                   | $0 \sim \infty$       | 中間層、隠れ層 |
| ソフトマックス | $\frac{e^{x_i}}{\sum e^{x_j}}$ | $0 \sim 1$（合計1）       | 多クラス分類  |


### 出力ユニットと損失関数の関係

| タスク種類  | 出力ユニット | 活性化関数   | 損失関数例           |
| ------ | ------ | ------- | --------------- |
| 回帰     | 1または複数 | なし      | 平均二乗誤差（MSE）など   |
| 二値分類   | 1      | シグモイド   | バイナリクロスエントロピー   |
| 多クラス分類 | クラス数   | ソフトマックス | カテゴリカルクロスエントロピー |


:::message
**損失関数：** 機械学習モデルの予測と正解（教師データ）との「ズレ（誤差）」を数値化する関数です。
このズレの大きさ（損失）を最小化するようにモデルを学習（パラメータの更新）していきます
:::