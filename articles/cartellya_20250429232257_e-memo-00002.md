---
title: "情報理論：情報量とエントロピー"
emoji: "✅"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["学習記録"]
published: true
---
## はじめに
:::message alert
記事内容は筆者の勉強まとめ内容です。内容の正確さや漏れについては保証できません。
ご覧いただく際は、ご自身で判断のうえ参考にしてください。
:::


## 概要
- シラバス：E資格2024#2
- 情報理論の自己情報量とエントロピー、及び数値化する方法を学習します

## キーワード
自己情報量, 相互情報量,付加情報, エントロピー, 
シャノンエントロピー
## 学習内容

### 情報量
ある事象が起こったとき、その事象がどれだけ「意外」かを数値化したことです

- 起こりやすい事象の情報量は少ない　→　明日太陽が爆発しない
    - 間違いなく起こる事は情報量がゼロ 。
- 起こりにくい事象ほど、その情報量は多い。　→　明日太陽が爆発する
- 独立な事象は、付加情報を持つ
    - コインを投げて一回表がでるより、二回投げて二回表が出る方が情報量が多いです

#### 自己情報量
とある事象が起きたとき、その事象がどれだけの情報をもたらすか（どれだけ意外か）
例：ある道路において、朝8時の「ある道路が渋滞するかどうか」予測します

#### 相互情報量
ある情報Yを追加で与えたときに、元の情報Xに関する不確実性がどれだけ減るか
例：ある道路において、雨の日の朝8時の「ある道路が渋滞するかどうか」予測します

#### 付加情報
相互情報量を通して、情報理論的に明確に測ることができる
例：相互情報量の例で、雨の日が付加情報です

#### 例まとめ
> 朝8時のある道路が渋滞するかどうか
> 仮定：
> 　毎日8時にその道路が渋滞する確率：0.8
> 　毎日8時にその道路が渋滞しない確率：0.2

- 結果1：今日渋滞する　→　想定通りで、情報量が少ない
- 結果2：今日渋滞しない　→　珍しい出来ことなので、情報量が大きい（意外性）

> 仮定追加：
> 　曜日か渋滞と関係ない

付加情報候補は下記です
- 天気：雨の日は交通量が増えることが多い
- 曜日：どの曜日でも交通量大差がない
- 付近のイベント情報：イベントがあるとき渋滞が発生しやすい

結果
- 雨の日の朝8時の道路が渋滞するかどうか　→　付加情報より、渋滞する確率が上がることを予測できます
- 日曜日の朝8時の道路が渋滞するかどうか　→　付加情報より、渋滞する確率が変わりません（曜日と渋滞状況は関係ない）
- 展示会がある日の朝8時の道路が渋滞するかどうか　→　付加情報より、渋滞する確率が上がることを予測できます


### 情報量を数値化
#### 自己情報量
$I(x)=-logP(x)$

- 底がeの場合は、情報の単位はナット(nats)
- 底が2の場合は、情報の単位はビット(bits)、もしくはシャノンと呼ばれます
- P(1)のとき、I(x)=0 （情報量なし）
- $\lim\limits_{x\to+0} P(x)$のとき、$I(x)\to\infin$ （情報量大）

#### エントロピー
- 自己情報量を平均してどれだけ情報が得られますか。言い換えると、自己情報量の期待値のことです
- 底が2の場合時、シャノンエントロピーと呼べます
- 確率が高い事象ばかりなら：予測しやすい → 情報量が少ない → エントロピーは小さい
- 確率がバラけているなら：予測しづらい → 情報量が多い → エントロピーは大きい

エントロピーの値に関して
- エントロピーが最大になるのは、すべての事象が同じ確率（＝完全にランダム）なときです
- 偏っていればいるほど、エントロピーは小さくなります


$$
\begin{aligned}
H(x)&=\mathbb{E}_{H\text{\textasciitilde}P}[I(x)]=-\mathbb{E}_{H\text{\textasciitilde}P}[logP(x)]\\
&=-\displaystyle\sum_{x=1}^n P(x)logP(x)
\end{aligned}
$$
:::message
Eは期待値
pは独立な情報源のそれぞれの事象が起こる確率
:::

#### 例まとめ
- X：渋滞
    - 仮に、渋滞ありをAとして、渋滞ありの確率はp(A)、渋滞あり(A)の自己情報量はI(A)
    - 仮に、渋滞なしをBとして、渋滞なしの確率はp(B)、渋滞なし(B)の自己情報量はI(B)
- エントロピーH(X)：渋滞の「予測の難しさ」
- 条件付きエントロピーH(X|Y)：Yを知ったあとも残る「予測の難しさ」
- 相互情報量I(X;Y)=H(X)-H(X|Y)：Yによってどれだけ助かったか
    - I(渋滞;曜日)=0 →　曜日と渋滞無関係
    - I(渋滞;天気)>0 →　雨の日に渋滞しやすい

:::message
H(X|Y)：事象Y（雨の日、日曜日など）が起きた後で事象X（渋滞）が発生する状態
詳細はベイズ則を参照します
:::

### その他勉強資料
https://www.momoyama-usagi.com/entry/info-entropy